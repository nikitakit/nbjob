{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbjob.jobdb\n",
    "Routines for interacting with the MongDB job database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only nbjob developers should see this message.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # When this file is run as part of the nbjob package, relative imports are allowed\n",
    "    from .structs import Struct, impl, init\n",
    "    from . import ipyparallel_display\n",
    "except SystemError:\n",
    "    # This file is a Jupyter notebook precisely to allow opening it in Jupyter\n",
    "    # during development. Don't use relative imports here\n",
    "    import notebook_import_hooks\n",
    "    from nbjob_dev.structs import Struct, impl, init\n",
    "    import nbjob_dev.ipyparallel_display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core data structures\n",
    "\n",
    "Objects in JobDB fall into five categories\n",
    "* Entries - linked list corresponding to the source of a job, plus any checkpoints\n",
    "* Jobs\n",
    "* Analyses\n",
    "* Analysis_entries - results of analyses run. These are not exposed directly, but can be accessed through the `Analysis` class\n",
    "* fs - There is a GridFS filesystem that stores arbitrary binary objects, such as dumps of model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from bson.objectid import ObjectId\n",
    "import gridfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class JobDB(metaclass=Struct):\n",
    "    \"\"\"\n",
    "    A wrapper around a MongoDB database that is used for nbjob.\n",
    "    \"\"\"\n",
    "    def __init__(self, db=None):\n",
    "        if db is not None:\n",
    "            self.db = db\n",
    "            self.client = db.client\n",
    "        else:\n",
    "            # Connect to the database on localhost\n",
    "            self.client = MongoClient()\n",
    "            self.db = self.client.db\n",
    "        \n",
    "        # Key collections\n",
    "        self.entries = self.db.entries_test\n",
    "        self.jobs = self.db.jobs_test\n",
    "        self.analysis_entries = self.db.analysis_entries_test\n",
    "        self.analyses = self.db.analyses_test\n",
    "        \n",
    "        self.fs = gridfs.GridFSBucket(self.db)\n",
    "        \n",
    "        # Entry history cache\n",
    "        # Iterating over entry history is slow for jobs with a long history,\n",
    "        # so we can cache history results here.\n",
    "        self.entry_history_cache={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Entry(object):\n",
    "    \"\"\"\n",
    "    'entries' are a linked list of checkpoints and snippets of executable code.\n",
    "    They are the core datastructure for storing source code in the system.\n",
    "    \"\"\"\n",
    "    \n",
    "    TYPE_SRC = 'src'\n",
    "    TYPE_CHECKPOINT = 'checkpoint'\n",
    "    TYPE_BINARY = 'binary' # deprecated, use 'file' instead\n",
    "    TYPE_FILE = 'file'\n",
    "    valid_types = [TYPE_SRC, TYPE_CHECKPOINT, TYPE_BINARY, TYPE_FILE]\n",
    "    \n",
    "    def __init__(self, jobdb, _id=None):\n",
    "        \"\"\"\n",
    "        Get or create an entry in the database\n",
    "        \n",
    "        If _id is None, creates a new root entry (of type 'checkpoint')\n",
    "        Otherwise, retreives an entry from the database\n",
    "        \"\"\"\n",
    "        self.jobdb = jobdb\n",
    "        \n",
    "        if _id is None:\n",
    "            res = self.jobdb.entries.insert_one({\n",
    "                    'type' : Entry.TYPE_CHECKPOINT,\n",
    "                    'names' : [],\n",
    "                    'prev' : None\n",
    "                })\n",
    "            self._id = res.inserted_id\n",
    "        else:\n",
    "            if isinstance(_id, str):\n",
    "                _id = ObjectId(_id)\n",
    "            self._id = _id\n",
    "            \n",
    "    def __eq__(self, other):\n",
    "        return type(other) is type(self) and other._id == self._id and other.jobdb == self.jobdb\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(type(self)) ^ hash(self._id)\n",
    "    \n",
    "    @property\n",
    "    def prev(self):\n",
    "        prev_id = self.jobdb.entries.find_one({'_id' : self._id}, {'prev' : True})['prev']\n",
    "        if prev_id is None:\n",
    "            return None\n",
    "        else:\n",
    "            return Entry(self.jobdb, prev_id)\n",
    "    \n",
    "    @property\n",
    "    def type(self):\n",
    "        res = self.jobdb.entries.find_one({'_id' : self._id}, {'type' : True})['type']\n",
    "        assert (res in Entry.valid_types)\n",
    "        return res\n",
    "    \n",
    "    @property\n",
    "    def names(self):\n",
    "        res = self.jobdb.entries.find_one({'_id' : self._id}, {'names' : True})['names']\n",
    "        res = set(res)\n",
    "        return res\n",
    "    \n",
    "    @property\n",
    "    def src(self):\n",
    "        if self.type != Entry.TYPE_SRC:\n",
    "            return \"\"\n",
    "        res = self.jobdb.entries.find_one({'_id' : self._id}, {'src' : True})['src']\n",
    "        return res\n",
    "    \n",
    "    # All types\n",
    "    def new(self, *args, **kwargs):\n",
    "        arg1, *_ = args\n",
    "        if isinstance(arg1, list):\n",
    "            return self._new_history(*args, **kwargs)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def _new_history(self, history):\n",
    "        current = self\n",
    "        for names, src in history:\n",
    "            current = current._new_entry(type=Entry.TYPE_SRC, names=names, src=src)\n",
    "        return current\n",
    "    \n",
    "    def _new_entry(self, type, names=set(), src=\"\"):\n",
    "        d = {}\n",
    "        if type == Entry.TYPE_SRC:\n",
    "            if not (isinstance(names, set) and isinstance(src, str)):\n",
    "                raise ValueError()\n",
    "            d['type'] = type\n",
    "            d['names'] = list(names)\n",
    "            d['src'] = src\n",
    "            d['prev'] = self._id\n",
    "        elif type == Entry.TYPE_CHECKPOINT:\n",
    "            d['type'] = type\n",
    "            if names != set():\n",
    "                raise ValueError()\n",
    "            d['names'] = []\n",
    "            d['prev'] = self._id\n",
    "        else:\n",
    "            raise ValueError(\"Invalid type\")\n",
    "            \n",
    "        new_id = self.jobdb.entries.insert_one(d).inserted_id\n",
    "        return Entry(self.jobdb, new_id)\n",
    "    \n",
    "    def checkpoint(self):\n",
    "        return self._new_entry(type=Entry.TYPE_CHECKPOINT)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        if self._id is None:\n",
    "            return \"Entry()\"\n",
    "        else:\n",
    "            return repr(self._id).replace('ObjectId', 'Entry')\n",
    "        \n",
    "    @property\n",
    "    def history(self):\n",
    "        res = []\n",
    "        current = self\n",
    "        num_checkpoints = 0\n",
    "        checkpoint_names = set()\n",
    "        cache_result = False\n",
    "        \n",
    "        while current is not None:\n",
    "            if current in self.jobdb.entry_history_cache:\n",
    "                # Cache results of iterating history\n",
    "                cached_value = self.jobdb.entry_history_cache[current]\n",
    "                if num_checkpoints > 0 and cached_value and '#Omitted' in cached_value[0][1]:\n",
    "                    try:\n",
    "                        cached_num_checkpoints = int(cached_value[0][1].split('Omitted')[1].split()[1])\n",
    "                        num_checkpoints += cached_num_checkpoints\n",
    "                        cached_value[0] = (cached_value[0][0] | checkpoint_names,\n",
    "                                           '#Omitted {} checkpoints'.format(num_checkpoints))\n",
    "                        if not res:\n",
    "                            res = cached_value\n",
    "                        else:\n",
    "                            res.extend(cached_value)\n",
    "                        break\n",
    "                    except:\n",
    "                        pass # Can't use cache\n",
    "                else:\n",
    "                    if not res:\n",
    "                        res = cached_value\n",
    "                    else:\n",
    "                        res.extend(cached_value)\n",
    "                    break\n",
    "            if current.type == Entry.TYPE_SRC:\n",
    "                if num_checkpoints or checkpoint_names:\n",
    "                    if num_checkpoints > 30:\n",
    "                        cache_result = True\n",
    "                    res.append((checkpoint_names, '#Omitted {} checkpoints'.format(num_checkpoints)))\n",
    "                    num_checkpoints = 0\n",
    "                    checkpoint_names = set()\n",
    "                res.append((current.names, current.src))\n",
    "            elif current.type == Entry.TYPE_CHECKPOINT:\n",
    "                num_checkpoints += 1\n",
    "            elif current.type == Entry.TYPE_BINARY:\n",
    "                checkpoint_names |= current.names\n",
    "            elif current.type == Entry.TYPE_FILE:\n",
    "                checkpoint_names |= current.names\n",
    "            current = current.prev\n",
    "        \n",
    "        if cache_result or len(res) > 50:\n",
    "            self.jobdb.entry_history_cache[self] = res\n",
    "        \n",
    "        res = res[::-1]\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Job(metaclass=Struct):\n",
    "    def __init__(self, jobdb, _id):\n",
    "        if isinstance(_id, str):\n",
    "            _id = ObjectId(_id)\n",
    "        self.jobdb = jobdb\n",
    "        self._id = _id\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        return type(other) is type(self) and other._id == self._id and other.jobdb == self.jobdb\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(type(self)) ^ hash(self._id)\n",
    "    \n",
    "    @property\n",
    "    def entry(self):\n",
    "        return Entry(self.jobdb,\n",
    "                     self.jobdb.jobs.find_one({'_id': self._id}, {'entry' : True})['entry'])\n",
    "    \n",
    "    @property\n",
    "    def info(self):\n",
    "        res = self.jobdb.jobs.find_one({'_id': self._id})\n",
    "        if res['entry'] is not None:\n",
    "            res['entry'] = Entry(self.jobdb, res['entry'])\n",
    "            \n",
    "        if 'tags' not in res:\n",
    "            res['tags'] = []\n",
    "        return res\n",
    "    \n",
    "    @property\n",
    "    def analyses(self):\n",
    "        for a_record in self.jobdb.analyses.find({'job': self._id}, {'_id' : True}):\n",
    "            yield Analysis(self.jobdb, a_record['_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@impl(Job)\n",
    "def rename(self, new_name):\n",
    "    self.jobdb.jobs.update_one({'_id': self._id},\n",
    "                               {'$set': {'name': new_name,\n",
    "                                         }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@impl(Job)\n",
    "def change_tags(self, new_tags):\n",
    "    self.jobdb.jobs.update_one({'_id': self._id},\n",
    "                               {'$set': {'tags': new_tags,\n",
    "                                     }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Analysis(metaclass=Struct):\n",
    "    def __init__(self, jobdb, _id):\n",
    "        if isinstance(_id, str):\n",
    "            _id = ObjectId(_id)\n",
    "        self.jobdb = jobdb\n",
    "        self._id = _id\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        return type(other) is type(self) and other._id == self._id and other.jobdb == self.jobdb\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(type(self)) ^ hash(self._id)\n",
    "       \n",
    "    @property\n",
    "    def job(self):\n",
    "        return Job(self.jobdb,\n",
    "                   self.jobdb.analyses.find_one({'_id': self._id}, {'job' : True})['job'])\n",
    "    \n",
    "    @property\n",
    "    def info(self):\n",
    "        res = self.jobdb.analyses.find_one({'_id': self._id})\n",
    "        res['job'] = Job(self.jobdb, res['job'])\n",
    "        return res\n",
    "    \n",
    "    @property\n",
    "    def results(self):\n",
    "        info = self.info\n",
    "        \n",
    "        current_id = info['analysis_entry']\n",
    "        while current_id is not None:\n",
    "            current = self.jobdb.analysis_entries.find_one({'_id': current_id})\n",
    "            current_id = current['prev']\n",
    "            del current['prev'], current['_id']\n",
    "            if 'checkpoint' in current and current['checkpoint'] is not None:\n",
    "                current['checkpoint'] = Entry(self.jobdb, current['checkpoint'])\n",
    "            yield current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@impl(Analysis)\n",
    "def rename(self, new_name):\n",
    "    self.jobdb.analyses.update_one({'_id': self._id},\n",
    "                                   {'$set': {'name': new_name,\n",
    "                                         }})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, signal, psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@impl(Job)\n",
    "def check_alive(self):\n",
    "    \"\"\"\n",
    "    Check if job is alive.\n",
    "    \n",
    "    If a job is discovered to be dead, it updates the job record occordingly.\n",
    "    \"\"\"\n",
    "    info = self.info\n",
    "    \n",
    "    if info['stop_time'] is not None:\n",
    "        return False\n",
    "    else:\n",
    "        res = psutil.pid_exists(self.info['pid'])\n",
    "        if not res:\n",
    "            self.jobdb.jobs.update_one({'_id': self._id},\n",
    "                                       {'$set': {'stop_time': 'unknown',\n",
    "                                                 'status': 'Dead'}})\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@impl(Analysis)\n",
    "def check_alive(self):\n",
    "    \"\"\"\n",
    "    Check if an analyzer is alive.\n",
    "    \n",
    "    If a analyzer is discovered to be dead, it updates the record occordingly.\n",
    "    \"\"\"\n",
    "    info = self.info\n",
    "    \n",
    "    if info['stop_time'] is not None:\n",
    "        return False\n",
    "    else:\n",
    "        res = psutil.pid_exists(self.info['pid'])\n",
    "        if not res:\n",
    "            self.jobdb.analyses.update_one({'_id': self._id},\n",
    "                                             {'$set': {'stop_time': 'unknown',\n",
    "                                                       'status': 'Dead'}})\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@impl(Job)\n",
    "def kill(self):\n",
    "    pid = self.info['pid']\n",
    "    \n",
    "    if not check_alive(self):\n",
    "        return\n",
    "    \n",
    "    os.kill(pid, signal.SIGINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@impl(Analysis)\n",
    "def kill(self):\n",
    "    pid = self.info['pid']\n",
    "    \n",
    "    if not check_alive(self):\n",
    "        return\n",
    "    \n",
    "    os.kill(pid, signal.SIGINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffing entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unified_diff(a, b):\n",
    "    if not isinstance(a, Entry) or not isinstance(b, Entry):\n",
    "        raise ValueError\n",
    "        \n",
    "    a = [x[1] for x in a.history]\n",
    "    b = [x[1] for x in b.history]\n",
    "    s = difflib.SequenceMatcher(a=a, b=b)\n",
    "\n",
    "    starta = 0\n",
    "    startb = 0\n",
    "    matches = s.get_matching_blocks()\n",
    "    non_matches = []\n",
    "    while True:\n",
    "        match = matches.pop()\n",
    "        enda = match.a\n",
    "        endb = match.b\n",
    "        non_matches.append((a[starta:enda], b[startb:endb]))\n",
    "        if match.size == 0:\n",
    "            break\n",
    "        starta = enda + match.size\n",
    "        startb = endb + match.size\n",
    "\n",
    "    for va, vb in non_matches:\n",
    "        for line in difflib.unified_diff('\\n'.join(va).splitlines(True), '\\n'.join(vb).splitlines(True)):\n",
    "            yield line\n",
    "\n",
    "Entry.unified_diff = unified_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dashboard code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ipywidgets as ipw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EntrySource(ipw.FlexBox):\n",
    "    def __init__(self, entry):\n",
    "        super().__init__()\n",
    "        self.entry = entry\n",
    "        self.refresh()\n",
    "        #self.children = [ipw.Button(description=\"Show\")]\n",
    "        #self.children[0].on_click(lambda *_: self.refresh())\n",
    "    \n",
    "    def refresh(self):\n",
    "        html = \"\".join([\"<pre>\" + el + \"</pre>\" for (_, el) in self.entry.history])\n",
    "        self.children = (ipw.HTML(html),)\n",
    "\n",
    "#EntrySource(job.entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EntryDiff(ipw.FlexBox):\n",
    "    def __init__(self, entry_a, entry_b):\n",
    "        super().__init__()\n",
    "        self.entry_a = entry_a\n",
    "        self.entry_b = entry_b\n",
    "        self.refresh()\n",
    "        #self.children = [ipw.Button(description=\"Show\")]\n",
    "        #self.children[0].on_click(lambda *_: self.refresh())\n",
    "    \n",
    "    def refresh(self):\n",
    "        html = \"\"\n",
    "        style = \"border:none; padding:1px; margin:0px;\"\n",
    "        for line in Entry.unified_diff(self.entry_a, self.entry_b):\n",
    "            if line.startswith('-'):\n",
    "                s = '<span style=\"color:red\">{}</span>'\n",
    "            elif line.startswith('+'):\n",
    "                s = '<span style=\"color:green\">{}</span>'\n",
    "            elif line.startswith('@'):\n",
    "                s = '<span style=\"color:purple\">{}</span>'\n",
    "            else:\n",
    "                s = '{}'\n",
    "            html += ('<pre style=\"{}\">' + s + '</pre>').format(style, line)\n",
    "        self.children = (ipw.HTML(html),)\n",
    "\n",
    "#EntryDiff(job.entry, job2.entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class JobInfo(ipw.FlexBox):\n",
    "    def __init__(self, job):\n",
    "        super().__init__()\n",
    "        self.job = job\n",
    "        self.refresh()\n",
    "        \n",
    "    def refresh(self):\n",
    "        job_alive = self.job.check_alive() # this can modify info, so do it first\n",
    "        info = self.job.info\n",
    "        job_history = self.job.entry.history\n",
    "        \n",
    "        children = []\n",
    "        \n",
    "        children.append(ipw.HTML(\"\"\"\n",
    "            <h3> Job Information </h3>\n",
    "            <p> Id: <span style=\"font-family:monospace\">{_id}</span></p>\n",
    "        \"\"\".format(**info)))\n",
    "        \n",
    "        self.name_field = ipw.Text(value=info['name'])\n",
    "        rename_button = ipw.Button(description='Rename')\n",
    "        rename_button.on_click(self.rename)\n",
    "        \n",
    "        children.append(ipw.HBox((\n",
    "                    ipw.HTML(\"Name:\"),\n",
    "                    self.name_field,\n",
    "                    rename_button,\n",
    "                )))\n",
    "        \n",
    "        self.tags_field = ipw.Text(value=' '.join(info['tags']))\n",
    "        change_tags_button = ipw.Button(description='Change')\n",
    "        change_tags_button.on_click(self.change_tags)\n",
    "        \n",
    "        children.append(ipw.HBox((\n",
    "                    ipw.HTML('Tags:'),\n",
    "                    self.tags_field,\n",
    "                    change_tags_button,\n",
    "                )))\n",
    "        \n",
    "        # TODO: non-hacky way finding number of checkpoints\n",
    "        s = job_history[-1][1].split('Omitted')\n",
    "        if len(s) == 2:\n",
    "            num_checkpoints = s[1].split()[0]\n",
    "        else:\n",
    "            num_checkpoints = 'unknown'\n",
    "        \n",
    "        children.append(ipw.HTML(\"\"\"\n",
    "            <h4> Status </h4>\n",
    "            <p> {status} </p>\n",
    "            <p> {start_time} &#8212; {stop_time} </p>\n",
    "            <p> Number of checkpoints: {num_checkpoints} </p>\n",
    "        \"\"\".format(num_checkpoints=num_checkpoints, **info)))\n",
    "        \n",
    "        \n",
    "        children.append(ipw.HTML(\"\"\"\n",
    "            <h4> Parameters </h4>\n",
    "        \"\"\"))\n",
    "        \n",
    "        parameter_srcs = [x for x in job_history if '_ParamContainer' in x[1]]\n",
    "        for names, src in parameter_srcs:\n",
    "            name = list(names)[0]\n",
    "            pairs = []\n",
    "            for line in src.splitlines():\n",
    "                if line.startswith(name + '.'):\n",
    "                    line = line.split(name, 1)[1][1:]\n",
    "                    var, value = line.split('=', 1)\n",
    "                    pairs.append((var, value))\n",
    "\n",
    "            children.append(ipw.HBox((\n",
    "                        ipw.HTML('<p style=\"font-family:monospace;\">{}.</pre>'.format(name)),\n",
    "                        ipw.HTML(\n",
    "                            \"\\n\".join(['<p style=\"font-family:monospace\">{}={}</p>'.format(*x)\n",
    "                                       for x in sorted(pairs)])\n",
    "                        )\n",
    "                    )))\n",
    "        \n",
    "        if job_alive:\n",
    "            children.append(ipw.HTML(\"\"\"\n",
    "                <h4> Machine info </h4>\n",
    "                <p> User: {user} </p>\n",
    "                <p> Hostname: {hostname} </p>\n",
    "                <p> PID: {pid} </p>\n",
    "            \"\"\".format(**info)))\n",
    "            \n",
    "            kill_button = ipw.Button(description='Kill')\n",
    "            kill_button.on_click(self.kill_job)\n",
    "            children.append(kill_button)\n",
    "        \n",
    "        \n",
    "        # Display\n",
    "        self.children = children\n",
    "    \n",
    "    def rename(self, *_):\n",
    "        new_name = self.name_field.value\n",
    "        self.job.rename(new_name)\n",
    "        self.refresh()\n",
    "        \n",
    "    def change_tags(self, *_):\n",
    "        new_tags = sorted(set(self.tags_field.value.split(' ')))\n",
    "        self.job.change_tags(new_tags)\n",
    "        self.refresh()\n",
    "    \n",
    "    def kill_job(self, *_):\n",
    "        self.job.kill()\n",
    "\n",
    "\n",
    "#JobInfo(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AnalysisInfo(ipw.FlexBox):\n",
    "    def __init__(self, analysis):\n",
    "        super().__init__()\n",
    "        self.analysis = analysis\n",
    "        self.refresh()\n",
    "        \n",
    "    def refresh(self):\n",
    "        analysis_alive = self.analysis.check_alive() # this can modify info, so do it first\n",
    "        info = self.analysis.info\n",
    "        \n",
    "        results = list(self.analysis.results)\n",
    "        if results:\n",
    "            keys = set.union(*[set(x.keys()) for x in results])\n",
    "            keys -= set(['_id', 'checkpoint', 'prev'])\n",
    "        else:\n",
    "            keys = set()\n",
    "        keys = sorted(keys)\n",
    "        \n",
    "        children = []\n",
    "        \n",
    "        children.append(ipw.HTML(\"\"\"\n",
    "            <h3> Analysis Information </h3>\n",
    "            <p> Id: <span style=\"font-family:monospace\">{_id}</span></p>\n",
    "            <p> Job id: <span style=\"font-family:monospace\">{job._id}</span></p>\n",
    "        \"\"\".format(**info)))\n",
    "        \n",
    "        self.name_field = ipw.Text(value=info['name'])\n",
    "        rename_button = ipw.Button(description='Rename')\n",
    "        rename_button.on_click(self.rename)\n",
    "        \n",
    "        children.append(ipw.HBox((\n",
    "                    ipw.HTML(\"Name:\"),\n",
    "                    self.name_field,\n",
    "                    rename_button,\n",
    "                )))\n",
    "        \n",
    "        children.append(ipw.HTML(\"\"\"\n",
    "            <h4> Status </h4>\n",
    "            <p> {status} </p>\n",
    "            <p> {start_time} &#8212; {stop_time} </p>\n",
    "            <p> Number of checkpoints: {num_checkpoints} </p>\n",
    "        \"\"\".format(num_checkpoints=len(results), **info)))\n",
    "        \n",
    "        var_text = \"<h4> Variables </h4>\"\n",
    "        for key in keys:\n",
    "            types = set([type(x.get(key, None)) for x in results[-50:]])\n",
    "            var_text += \"\"\"<p style=\"font-family=monospace\">{} : {}</pre>\"\"\".format(\n",
    "                key, \", \".join(t.__name__ for t in types))\n",
    "        \n",
    "        children.append(ipw.HTML(var_text))\n",
    "        \n",
    "        if analysis_alive:\n",
    "            children.append(ipw.HTML(\"\"\"\n",
    "                <h4> Machine info </h4>\n",
    "                <p> User: {user} </p>\n",
    "                <p> Hostname: {hostname} </p>\n",
    "                <p> PID: {pid} </p>\n",
    "            \"\"\".format(**info)))\n",
    "            \n",
    "            kill_button = ipw.Button('Kill')\n",
    "            kill_button.on_click(self.kill_analysis)\n",
    "            children.append(kill_button)\n",
    "        \n",
    "        \n",
    "        # Display\n",
    "        self.children = children\n",
    "    \n",
    "    def rename(self, *_):\n",
    "        new_name = self.name_field.value\n",
    "        self.analysis.rename(new_name)\n",
    "        self.refresh()\n",
    "    \n",
    "    def kill_analysis(self, *_):\n",
    "        self.analysis.kill()\n",
    "\n",
    "\n",
    "#AnalysisInfo(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Listings(ipw.FlexBox):\n",
    "    def __init__(self, iterable, on_hide, calc_status, on_second, on_info):\n",
    "        super().__init__(orientation='vertical')\n",
    "        listings = []\n",
    "        for item in iterable:\n",
    "            listing = ipw.HBox()\n",
    "            hide_button = ipw.Button(description='X', button_style='danger')\n",
    "            status_line = ipw.HTML(\"\"\"<p style=\"width:12em; overflow:hidden; font-family:monospace\">{}</p>\"\"\".format(\n",
    "                calc_status(item)))\n",
    "            status_line.margin = \"5px\"\n",
    "            src_button = ipw.Button(description = '2nd')\n",
    "            info_button = ipw.Button(description = 'info')\n",
    "            listing.children += (hide_button, status_line, src_button, info_button)\n",
    "            listing.padding = \"2px\"\n",
    "\n",
    "            hide_button._item = item\n",
    "            hide_button._listing = listing\n",
    "            hide_button.on_click(lambda x: on_hide(x._item, x._listing))\n",
    "            src_button._item = item\n",
    "            #src_button._listing = listing\n",
    "            src_button.on_click(lambda x: on_second(x._item))\n",
    "            info_button._item = item\n",
    "            #info_button._listing = listing\n",
    "            info_button.on_click(lambda x: on_info(x._item))\n",
    "            listings.append(listing)\n",
    "        \n",
    "        self.children = listings\n",
    "\n",
    "Listings(['a', 'b', 'c'],\n",
    "             lambda _, x:(None, x.close()),\n",
    "             lambda x: x,\n",
    "             lambda x:None,\n",
    "             lambda x:None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SourceTab(ipw.FlexBox):\n",
    "    def __init__(self, primary, secondary=None):\n",
    "        super().__init__()\n",
    "        self.primary = primary\n",
    "        self.secondary = secondary\n",
    "        self.suggestions = []\n",
    "        \n",
    "        if secondary is not None:\n",
    "            self.suggestions.append(secondary)\n",
    "        \n",
    "        self.refresh()\n",
    "    \n",
    "    def refresh(self):\n",
    "        if self.primary is None:\n",
    "            self.children = [ipw.HTML(\"No job selected\")]\n",
    "            return\n",
    "        children = []\n",
    "        options = [('None', None)]\n",
    "        \n",
    "        for suggestion in self.suggestions:\n",
    "            if suggestion == self.primary:\n",
    "                continue\n",
    "            options.append((suggestion.info['name'], suggestion))\n",
    "        \n",
    "        selected_label = 'None'\n",
    "        if self.secondary is not None:\n",
    "            assert self.secondary in self.suggestions\n",
    "            selected_label = self.secondary.info['name']\n",
    "        \n",
    "        self.picker = ipw.Dropdown(options=options, selected_label=selected_label)\n",
    "        \n",
    "        \n",
    "        children.append(ipw.HTML(\"\"\"\n",
    "            <h3> Job Source </h3>\n",
    "            <p> Id: <span style=\"font-family:monospace\">{_id}</span></p>\n",
    "            <p> Name: {name} </p>\n",
    "        \"\"\".format(**self.primary.info)))\n",
    "        \n",
    "        children.append(ipw.HBox((\n",
    "                    ipw.HTML('Compare with:'),\n",
    "                    self.picker\n",
    "                )))\n",
    "        \n",
    "        self.contents_proxy = ipw.Proxy(None)\n",
    "        \n",
    "        children.append(self.contents_proxy)\n",
    "        self.children = children\n",
    "        \n",
    "        self.picker.on_trait_change(self.picker_update)\n",
    "        self.refresh_contents()\n",
    "        \n",
    "    def picker_update(self, *_):\n",
    "        if self.picker.value != self.secondary:\n",
    "            self.secondary = self.picker.value\n",
    "            self.refresh_contents()\n",
    "    \n",
    "    def refresh_contents(self):\n",
    "        if self.secondary is None:\n",
    "            self.contents_proxy.child = EntrySource(self.primary.entry)\n",
    "        else:\n",
    "            self.contents_proxy.child = EntryDiff(self.secondary.entry, self.primary.entry)\n",
    "            \n",
    "    def set_primary(self, val):\n",
    "        if val == self.primary:\n",
    "            return\n",
    "        \n",
    "        if self.primary and self.primary not in self.suggestions:\n",
    "            self.suggestions.append(self.primary)\n",
    "        self.primary = val\n",
    "        self.secondary = None\n",
    "        \n",
    "        self.refresh()\n",
    "    \n",
    "    def set_secondary(self, val):\n",
    "        if val == self.secondary:\n",
    "            return\n",
    "        \n",
    "        if val not in self.suggestions:\n",
    "            self.suggestions.append(val)\n",
    "        \n",
    "        self.secondary = val\n",
    "        \n",
    "        self.refresh()\n",
    "\n",
    "SourceTab(None)\n",
    "#SourceTab(job, job2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AnalysisTab(ipw.FlexBox):\n",
    "    def __init__(self, job=None, analysis=None):\n",
    "        super().__init__()\n",
    "        self.job = job\n",
    "        self.analysis = None\n",
    "        \n",
    "        self.refresh()\n",
    "        \n",
    "    def refresh(self):\n",
    "        if self.job is None:\n",
    "            self.children = [ipw.HTML(\"No job selected\")]\n",
    "            return\n",
    "        \n",
    "        panel_1 = ipw.VBox()\n",
    "        panel_1.width = \"50%\"\n",
    "        panel_2 = ipw.VBox()\n",
    "        panel_2.width = \"50%\"\n",
    "        \n",
    "        self.column_1 = ipw.Proxy(None)\n",
    "        self.column_2 = ipw.Proxy(None)\n",
    "        \n",
    "        job_info_button = ipw.Button(description='Job info')\n",
    "        job_info_button.on_click(self.on_job_info_button)\n",
    "        \n",
    "        panel_1.children = [job_info_button, self.column_1]\n",
    "        panel_2.children = [self.column_2]\n",
    "        \n",
    "        container = ipw.HBox([panel_1, panel_2])\n",
    "        self.children = [container]\n",
    "        \n",
    "        self.refresh_contents()\n",
    "        \n",
    "    def refresh_contents(self):\n",
    "        if self.job is not None:\n",
    "            self.column_1.child = Listings(self.job.analyses,\n",
    "                 on_hide = lambda a, w: None,\n",
    "                 calc_status = lambda a: a.info['name'],\n",
    "                 on_second = self.on_analysis_second,\n",
    "                 on_info = self.on_analysis_info,\n",
    "                )\n",
    "            \n",
    "        if self.analysis is not None:\n",
    "            self.column_2.child = AnalysisInfo(self.analysis)\n",
    "        else:\n",
    "            self.column_2.child = JobInfo(self.job)\n",
    "    \n",
    "    def on_analysis_info(self, a):\n",
    "        if a != self.analysis:\n",
    "            self.analysis = a\n",
    "            self.refresh_contents()\n",
    "    \n",
    "    def on_analysis_second(self, a):\n",
    "        import __main__\n",
    "        __main__.analysis = a\n",
    "        self.on_analysis_info(a)\n",
    "        \n",
    "    def on_job_info_button(self, *_):\n",
    "        self.analysis = None\n",
    "        self.refresh_contents()\n",
    "        \n",
    "    def set_job(self, job):\n",
    "        if self.job == job:\n",
    "            return\n",
    "        \n",
    "        if self.job is None:\n",
    "            self.job = job\n",
    "            self.refresh()\n",
    "            return\n",
    "        self.job = job\n",
    "        self.analysis = None\n",
    "        self.refresh_contents()\n",
    "        \n",
    "AnalysisTab(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_jobs_query(db, query):\n",
    "    try:\n",
    "        res = list(db.jobs.find(query))\n",
    "    except Exception as e:\n",
    "        return \"{}\".format(e)\n",
    "    \n",
    "    return [Job(db, x['_id']) for x in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_jobs(db, id='', name='', tags='', only_running=False, exclude_status=''):\n",
    "    if id or name or tags or only_running or exclude_status:\n",
    "        pass\n",
    "    else:\n",
    "        return [], \"Please specify a search term\"\n",
    "    \n",
    "    \n",
    "    query = {}\n",
    "    if id:\n",
    "        if not isinstance(id, ObjectId):\n",
    "            try:\n",
    "                id = ObjectId(id)\n",
    "            except:\n",
    "                return [], \"Invalid id\"\n",
    "        query['_id'] = id\n",
    "        \n",
    "        res = run_jobs_query(db, query)\n",
    "        if isinstance(res, str):\n",
    "            return [], res\n",
    "        \n",
    "        if res:\n",
    "            msg = \"Found job matching id\"\n",
    "        else:\n",
    "            msg = \"Id not found\"\n",
    "        return res, msg\n",
    "    \n",
    "    if name:\n",
    "        query['name'] = re.compile(name)\n",
    "        \n",
    "    if tags:\n",
    "        query['tags'] = {'$all': tags.split()}\n",
    "        #return [], \"Tags are not yet supported\"\n",
    "    \n",
    "    if only_running:\n",
    "        query['stop_time'] = None\n",
    "    \n",
    "    if exclude_status:\n",
    "        query['status'] = {'$not': re.compile(exclude_status)}\n",
    "    \n",
    "    res = run_jobs_query(db, query)\n",
    "    if isinstance(res, str):\n",
    "        return [], res\n",
    "    \n",
    "    if only_running:\n",
    "        res = [x for x in res if x.check_alive()]\n",
    "    \n",
    "    return res, \"Found {} results\".format(len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class JobSearch(ipw.FlexBox):\n",
    "    def __init__(self, jobdb, on_job_info=lambda j: None, on_job_second=lambda j: None):\n",
    "        super().__init__()\n",
    "        self.jobdb = jobdb\n",
    "        self.on_job_info = on_job_info\n",
    "        self.on_job_second = on_job_second\n",
    "        self.refresh()\n",
    "    \n",
    "    def refresh(self):\n",
    "        self.id_field = ipw.Text()\n",
    "        self.name_field = ipw.Text()\n",
    "        self.tags_field = ipw.Text()\n",
    "        self.only_running_box = ipw.Checkbox()\n",
    "        self.exclude_status_field = ipw.Text()\n",
    "        \n",
    "        self.search_button = ipw.Button(description='Search')\n",
    "        self.status_area = ipw.HTML()\n",
    "        self.results_area = ipw.Proxy(None)\n",
    "        \n",
    "        for x in [self.id_field, self.name_field, self.tags_field, self.exclude_status_field]:\n",
    "            x.width = '66%'\n",
    "        \n",
    "        self.search_button.on_click(self.do_search)\n",
    "        \n",
    "        def to_html(txt):\n",
    "            return ipw.HTML('<p style=\"width:7em; text-align:right\">{}</p>'.format(txt))\n",
    "        \n",
    "        self.children = [\n",
    "            ipw.HBox((to_html('Id:'), self.id_field)),\n",
    "            ipw.HBox((to_html('Name:'), self.name_field)),\n",
    "            ipw.HBox((to_html('Tags:'), self.tags_field)),\n",
    "            ipw.HBox((to_html(''), self.only_running_box, ipw.HTML('&nbsp; Only running'))),\n",
    "            ipw.HBox((to_html('Exclude status:'), self.exclude_status_field)),\n",
    "            ipw.HTML('<hr style=\"width:100%\"/>'),\n",
    "            ipw.HBox((to_html(''), self.search_button)),\n",
    "            self.status_area,\n",
    "            self.results_area\n",
    "        ]\n",
    "    \n",
    "    def do_search(self, *_):\n",
    "        jobs, msg = find_jobs(self.jobdb,\n",
    "                              id = self.id_field.value,\n",
    "                              name = self.name_field.value,\n",
    "                              tags = self.tags_field.value,\n",
    "                              only_running = self.only_running_box.value,\n",
    "                              exclude_status = self.exclude_status_field.value\n",
    "                             )\n",
    "        \n",
    "        self.status_area.value = msg\n",
    "\n",
    "        \n",
    "        self.results_area.child = Listings(jobs,\n",
    "                                           on_hide = lambda j, w: w.close(),\n",
    "                                           calc_status = lambda j: j.info['name'],\n",
    "                                           on_second = self.on_job_second,\n",
    "                                           on_info = self.on_job_info,\n",
    "                                          )\n",
    "        \n",
    "#JobSearch(jobdb)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Dashboard(ipw.FlexBox):\n",
    "    def __init__(self, jobdb):\n",
    "        super().__init__(orientation='horizontal')\n",
    "        self.jobdb = jobdb\n",
    "        \n",
    "        self.refresh()\n",
    "        \n",
    "    def refresh(self):\n",
    "        panel_1 = ipw.VBox()\n",
    "        panel_1.width = \"33%\"\n",
    "        panel_2 = ipw.VBox()\n",
    "        panel_2.width = \"66%\"\n",
    "        \n",
    "        self.job_search = JobSearch(self.jobdb, on_job_second=self.set_secondary, on_job_info=self.set_primary)\n",
    "        self.analysis_tab = AnalysisTab(None)\n",
    "        self.source_tab = SourceTab(None)\n",
    "        \n",
    "        tabs = ipw.Tab([self.analysis_tab, self.source_tab])\n",
    "        tabs.set_title(0, 'Analysis')\n",
    "        tabs.set_title(1, 'Source')\n",
    "        \n",
    "        panel_1.children = [self.job_search]\n",
    "        panel_2.children = [tabs]\n",
    "        \n",
    "        self.children = [panel_1, panel_2]\n",
    "        \n",
    "    def set_primary(self, job):\n",
    "        self.source_tab.set_primary(job)\n",
    "        self.analysis_tab.set_job(job)\n",
    "    \n",
    "    def set_secondary(self, job):\n",
    "        self.source_tab.set_secondary(job)\n",
    "#Dashboard(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_default_dashboard():\n",
    "    jobdb = JobDB()\n",
    "    return Dashboard(jobdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The magic string below signals to the notebook importing machinery that\n",
    "# the examples section below should not be imported into any other modules\n",
    "\n",
    "#NBIMPORT_STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Dashboard(jobdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobdb = JobDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "job = Job(jobdb, '56301716006f87241d481184')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job2 = Job(jobdb, '563ee5ef006f872846cb6fd1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = difflib.SequenceMatcher(a=_153, b=_154)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Match(a=1, b=1, size=2),\n",
       " Match(a=6, b=4, size=2),\n",
       " Match(a=24, b=15, size=1),\n",
       " Match(a=26, b=18, size=1),\n",
       " Match(a=32, b=25, size=1),\n",
       " Match(a=35, b=27, size=0)]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.get_matching_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Match(a=1, b=1, size=2)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_165[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_166.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- \n",
      "+++ \n",
      "@@ -2,21 +2,19 @@\n",
      " class _ParamContainer(object): pass\n",
      " hparams = _ParamContainer()\n",
      " \n",
      "+hparams.batch_size = 10\n",
      "+\n",
      "+hparams.max_epochs = 40\n",
      "+\n",
      "+hparams.reverse_input = True\n",
      "+\n",
      "+hparams.dim_hidden = 400\n",
      "+\n",
      " hparams.learning_rate = 0.097\n",
      " \n",
      "-hparams.max_epochs = 20\n",
      "-\n",
      "-hparams.reverse_input = True\n",
      "-\n",
      "-hparams.dim_embeddings = 10\n",
      "-\n",
      "-hparams.patience = 500000\n",
      "-\n",
      "-hparams.dim_hidden = 50\n",
      "-\n",
      "-hparams.batch_size = 1\n",
      "-\n",
      "-hparams.frequency = 40\n",
      "+hparams.dim_embeddings = 400\n",
      "+\n",
      "+hparams.frequency = 4000\n",
      " \n",
      " # Imports for both remote engines and locally\n",
      " from theano import *\n",
      "@@ -31,11 +29,8 @@\n",
      " import stat, subprocess\n",
      " from collections import OrderedDict\n",
      " floatX = theano.config.floatX\n",
      "-theano.config.optimizer = 'fast_compile'\n",
      " theano.config.mode = 'FAST_RUN'\n",
      "-#theano.config.mode = 'DebugMode'\n",
      "-theano.config.DebugMode.check_py = False\n",
      "-theano.config.DebugMode.check_strides = False\n",
      "+theano.config.optimizer = 'fast_run'\n",
      " DATA_PREFIX = os.path.expanduser('~/data')\n",
      " def make_shared(datasets, x_cast=None, y_cast='int32'):\n",
      "     res = []\n",
      "@@ -49,472 +44,242 @@\n",
      "         res.append((set_x, set_y))\n",
      "     return tuple(res)\n",
      " \n",
      "-def minibatch_train(datasets,x,y,\n",
      "-          cost,\n",
      "-          updates,\n",
      "-          batch_size=1,\n",
      "-            frequency=1000,\n",
      "-         ):\n",
      "+class Optimizer(object):\n",
      "+    def __init__(self): pass\n",
      "+    \n",
      "+    @property\n",
      "+    def state_values(self):\n",
      "+        return [state_var.get_value() for state_var in self.state]\n",
      "+    \n",
      "+    @state_values.setter\n",
      "+    def state_values(self, values):\n",
      "+        for state_var, value in zip(self.state, values):\n",
      "+            state_var.set_value(value)    \n",
      "+\n",
      "+class SGDOptimizer(Optimizer):\n",
      "+    def __init__(self, cost, params, learning_rate=0.01):\n",
      "+        #self.updates = set()\n",
      "+        #for param in params:\n",
      "+        #    self.updates.add((param, param - learning_rate * T.grad(cost, param)))\n",
      "+        grads = T.grad(cost, params)\n",
      "+        self.updates = set(zip(param, param - learning_rate * g) for (param, g) in zip(params, grads))\n",
      "+        \n",
      "+        self.state = []\n",
      "+\n",
      "+class AdadeltaOptimizer(Optimizer):\n",
      "+    def __init__(self, cost, params, decay=0.95, epsilon=1e-6):\n",
      "+        grads = T.grad(cost, params)\n",
      "+        #grads = [T.grad(cost, param) for param in params]\n",
      "+            \n",
      "+        # Shared variables for storing discounted past gradients and updates\n",
      "+        running_grads2 = [theano.shared(np.asarray(param.get_value() * 0, dtype=floatX),\n",
      "+                                       name=\"{}_grads^2\".format(param.name))\n",
      "+                         for param in params]\n",
      "+            \n",
      "+        running_deltas2 = [theano.shared(np.asarray(param.get_value() * 0, dtype=floatX),\n",
      "+                                         name=\"{}_deltas^2\".format(param.name))\n",
      "+                           for param in params]\n",
      "+        \n",
      "+        new_running_grads2 = [(decay * old + (1. - decay) * (new ** 2))\n",
      "+                              for (old, new) in zip(running_grads2, grads)]\n",
      "+        \n",
      "+        deltas = [\n",
      "+            -tensor.sqrt(rd2 + epsilon) / tensor.sqrt(rg2 + epsilon) * g\n",
      "+            for (rg2, rd2, g) in zip(new_running_grads2,\n",
      "+                                     running_deltas2,\n",
      "+                                     grads)\n",
      "+            ]\n",
      "+        \n",
      "+        new_running_deltas2 = [(decay * old + (1. - decay) * (new ** 2))\n",
      "+                               for (old, new) in zip(running_deltas2, deltas)]\n",
      "+        \n",
      "+        self.updates = set((param, param + delta) for (param, delta) in zip(params, deltas))\n",
      "+        self.updates |= set(zip(running_grads2, new_running_grads2))\n",
      "+        self.updates |= set(zip(running_deltas2, new_running_deltas2))\n",
      "+        \n",
      "+        # State is also serialized, to allow resuming training\n",
      "+        self.state = running_grads2 + running_deltas2\n",
      "+\n",
      "+class MinibatchIterator(object):\n",
      "+    def __init__(self, datasets, x, y,\n",
      "+                  optimizer,\n",
      "+                  batch_size=1,\n",
      "+                  frequency=1000, #how often to yield to the checkpointer\n",
      "+                  ):\n",
      "+        (self.train_set_x, self.train_set_y), _, _ = datasets\n",
      "+        self.n_train_batches = self.train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
      "+            \n",
      "+        index = T.lscalar()\n",
      "+        self.train_model = theano.function(\n",
      "+                [index],\n",
      "+                [],\n",
      "+                updates=optimizer.updates,\n",
      "+                givens = {\n",
      "+                    x: self.train_set_x[index * batch_size : (index + 1) * batch_size],\n",
      "+                    y: self.train_set_y[index * batch_size : (index + 1) * batch_size]\n",
      "+                }\n",
      "+                )\n",
      "+            \n",
      "+        self.frequency = frequency\n",
      "+            \n",
      "+        self.epoch, self.minibatch_i = 1, 0\n",
      "+            \n",
      "+    def __iter__(self):\n",
      "+        # This is designed to be resumable\n",
      "+        while True:\n",
      "+            while self.minibatch_i < self.n_train_batches:\n",
      "+                self.train_model(self.minibatch_i)\n",
      "+                iteration = (self.epoch - 1) * self.n_train_batches + self.minibatch_i\n",
      "+                if (iteration + 1) % self.frequency == 0:\n",
      "+                    yield self.epoch, iteration\n",
      "+                self.minibatch_i += 1\n",
      "+            self.epoch += 1\n",
      "+            self.minibatch_i = 0\n",
      "+\n",
      "+    @property\n",
      "+    def state_values(self):\n",
      "+        return self.epoch, self.minibatch_i\n",
      "+\n",
      "+    @state_values.setter\n",
      "+    def state_values(self, val):\n",
      "+        self.epoch, self.minibatch_i = val\n",
      "+\n",
      "+class IndividualIterator(object):\n",
      "+    def __init__(self, datasets, x, y,\n",
      "+                  optimizer,\n",
      "+                  batch_size=1, # for compatibility, must be 1\n",
      "+                  frequency=1000, #how often to yield to the checkpointer\n",
      "+                  ):\n",
      "+        (self.train_set_x, self.train_set_y), _, _ = datasets\n",
      "+        \n",
      "+        assert batch_size==1\n",
      "+        if type(self.train_set_x) == list:\n",
      "+            self.n_train_batches = len(self.train_set_x) // batch_size\n",
      "+        else:\n",
      "+            self.n_train_batches = self.train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
      "+            self.train_set_x = self.train_set_x.get_value(borrow=True)\n",
      "+            self.train_set_y = self.train_set_y.eval()\n",
      "+        \n",
      "+        self.train_model = theano.function(\n",
      "+                [x, y],\n",
      "+                [],\n",
      "+                updates=optimizer.updates)\n",
      "+            \n",
      "+        self.frequency = frequency\n",
      "+            \n",
      "+        self.epoch, self.minibatch_i = 1, 0\n",
      "+            \n",
      "+    def __iter__(self):\n",
      "+        # This is designed to be resumable\n",
      "+        while True:\n",
      "+            while self.minibatch_i < self.n_train_batches:\n",
      "+                self.train_model([self.train_set_x[self.minibatch_i]],\n",
      "+                                 [self.train_set_y[self.minibatch_i]])\n",
      "+                iteration = (self.epoch - 1) * self.n_train_batches + self.minibatch_i\n",
      "+                if (iteration + 1) % self.frequency == 0:\n",
      "+                    yield self.epoch, iteration\n",
      "+                self.minibatch_i += 1\n",
      "+            self.epoch += 1\n",
      "+            self.minibatch_i = 0\n",
      "+\n",
      "+    @property\n",
      "+    def state_values(self):\n",
      "+        return self.epoch, self.minibatch_i\n",
      "+\n",
      "+    @state_values.setter\n",
      "+    def state_values(self, val):\n",
      "+        self.epoch, self.minibatch_i = val\n",
      "+\n",
      "+class MaskedMinibatchIterator(object):\n",
      "+    def __init__(self, training_set, x, x_mask, y, y_mask,\n",
      "+                  optimizer,\n",
      "+                  batch_size=1,\n",
      "+                  frequency=1000, #how often to yield to the checkpointer\n",
      "+                  ):\n",
      "+        self.set_x, self.mask_x, self.set_y, self.mask_y = training_set\n",
      "+        self.n_train_batches = self.set_x.shape[0].eval() // batch_size\n",
      "+        \n",
      "+        self.batch_size = batch_size\n",
      "+            \n",
      "+        index = T.lscalar()\n",
      "+        self.train_model = theano.function(\n",
      "+                [index],\n",
      "+                [],\n",
      "+                updates=optimizer.updates,\n",
      "+                givens = {\n",
      "+                    x: self.set_x[index * batch_size : (index + 1) * batch_size],\n",
      "+                    x_mask: self.mask_x[index * batch_size : (index + 1) * batch_size],\n",
      "+                    y: self.set_y[index * batch_size : (index + 1) * batch_size],\n",
      "+                    y_mask: self.mask_y[index * batch_size : (index + 1) * batch_size],\n",
      "+                }\n",
      "+                )\n",
      "+            \n",
      "+        self.frequency = frequency\n",
      "+            \n",
      "+        self.epoch, self.minibatch_i = 1, 0\n",
      "+            \n",
      "+    def __iter__(self):\n",
      "+        # This is designed to be resumable\n",
      "+        while True:\n",
      "+            while self.minibatch_i < self.n_train_batches:\n",
      "+                self.train_model(self.minibatch_i)\n",
      "+                iteration = ((self.epoch - 1) * self.n_train_batches + self.minibatch_i) * self.batch_size\n",
      "+                if (iteration + self.batch_size) % self.frequency == 0:\n",
      "+                    yield self.epoch, iteration\n",
      "+                self.minibatch_i += 1\n",
      "+            self.epoch += 1\n",
      "+            self.minibatch_i = 0\n",
      "+\n",
      "+    @property\n",
      "+    def state_values(self):\n",
      "+        return self.epoch, self.minibatch_i\n",
      "+\n",
      "+    @state_values.setter\n",
      "+    def state_values(self, val):\n",
      "+        self.epoch, self.minibatch_i = val\n",
      "+\n",
      "+class Checkpointer(object):\n",
      "     \"\"\"\n",
      "-    Train a model\n",
      "-    \n",
      "-    Generator\n",
      "+    Base checkpointer: checkpoint at every batch\n",
      "     \"\"\"\n",
      "-    (train_set_x, train_set_y), _, _ = datasets\n",
      "-    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
      "-    \n",
      "-    index = T.lscalar()\n",
      "-    train_model = theano.function(\n",
      "-        [index],\n",
      "-        cost,\n",
      "-        updates=updates,\n",
      "-        givens = {\n",
      "-            x: train_set_x[index * batch_size : (index + 1) * batch_size],\n",
      "-            y: train_set_y[index * batch_size : (index + 1) * batch_size]\n",
      "-        }\n",
      "-        )\n",
      "-    \n",
      "-    # Training procedure\n",
      "-    frequency = min(n_train_batches, frequency)\n",
      "-    epoch = 0\n",
      "-    while True:\n",
      "-        epoch += 1\n",
      "-        for minibatch_i in range(n_train_batches):\n",
      "-            minibatch_avg_cost = train_model(minibatch_i)\n",
      "-            iteration = (epoch - 1) * n_train_batches + minibatch_i\n",
      "-            if (iteration + 1) % frequency == 0:\n",
      "-                yield epoch, iteration\n",
      "-\n",
      "-def get_validator(datasets,x,y, error, batch_size=1):\n",
      "-    _, (valid_set_x, valid_set_y), _ = datasets\n",
      "-    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] // batch_size\n",
      "-    \n",
      "-    index = T.lscalar()\n",
      "-    validate_model = theano.function(\n",
      "-        [index],\n",
      "-        error,\n",
      "-        givens = {\n",
      "-            x: valid_set_x[index * batch_size : (index + 1) * batch_size],\n",
      "-            y: valid_set_y[index * batch_size : (index + 1) * batch_size]\n",
      "-        }    \n",
      "-    )\n",
      "-    \n",
      "-    def validate():\n",
      "-        validation_losses = [validate_model(i) for i in range(n_valid_batches)]\n",
      "-        return np.mean(validation_losses)\n",
      "-    \n",
      "-    return validate\n",
      "-\n",
      "-def get_tester(datasets,x,y, error, batch_size=1):\n",
      "-    _, _, (test_set_x, test_set_y) = datasets\n",
      "-    n_test_batches = test_set_x.get_value(borrow=True).shape[0] // batch_size\n",
      "-    \n",
      "-    index = T.lscalar()\n",
      "-    test_model = theano.function(\n",
      "-        [index],\n",
      "-        error,\n",
      "-        givens = {\n",
      "-            x: test_set_x[index * batch_size : (index + 1) * batch_size],\n",
      "-            y: test_set_y[index * batch_size : (index + 1) * batch_size]\n",
      "-        }    \n",
      "-    )\n",
      "-    \n",
      "-    def test():\n",
      "-        test_losses = [test_model(i) for i in range(n_test_batches)]\n",
      "-        return np.mean(test_losses)\n",
      "-    \n",
      "-    return test\n",
      "-\n",
      "-def patience_training(\n",
      "-        trainer, validate,\n",
      "-        params,\n",
      "-        patience = 100,\n",
      "-        patience_increase=2,\n",
      "-        improvement_threshold=0.995,\n",
      "-        max_epochs=1000\n",
      "-    ):\n",
      "-    best_validation_loss = np.inf\n",
      "-    for epoch, iteration in trainer:\n",
      "-        this_validation_loss = validate()\n",
      "-        if this_validation_loss < best_validation_loss:\n",
      "-            if this_validation_loss < best_validation_loss * improvement_threshold:\n",
      "-                patience = max(patience, iteration * patience_increase)\n",
      "-                print(\"Epoch {} iteration {}\".format(epoch, iteration))\n",
      "-            best_validation_loss = this_validation_loss\n",
      "-            yield epoch, iteration\n",
      "-\n",
      "-        print(\"Epoch {} current validation loss {}\".format(epoch, this_validation_loss))\n",
      "-        if patience <= iteration or epoch > max_epochs:\n",
      "-            break\n",
      "-\n",
      "-def individual_train(datasets, x, y, cost, updates, batch_size=1, frequency=1000, normalize=lambda: None):\n",
      "-    (train_set_x, train_set_y), _, _ = datasets\n",
      "-    \n",
      "-    assert batch_size==1\n",
      "-    if type(train_set_x) == list:\n",
      "-        n_train_batches = len(train_set_x) // batch_size\n",
      "-    else:\n",
      "-        n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
      "-        train_set_x = train_set_x.get_value(borrow=True)\n",
      "-        train_set_y = train_set_y.eval()\n",
      "-    \n",
      "-    train_model_individual = theano.function(\n",
      "-                    [x, y],\n",
      "-                    cost,\n",
      "-                    updates=updates)\n",
      "-    \n",
      "-    frequency = min(n_train_batches, frequency)\n",
      "-    epoch = 0\n",
      "-    while True:\n",
      "-        epoch += 1\n",
      "-        for minibatch_i in range(n_train_batches):\n",
      "-            for i in range(batch_size):\n",
      "-                train_model_individual([train_set_x[minibatch_i * batch_size + i]],\n",
      "-                                      [train_set_y[minibatch_i * batch_size + i]])\n",
      "-            normalize()\n",
      "-            \n",
      "-            iteration = (epoch - 1) * n_train_batches + minibatch_i\n",
      "-            if (iteration + 1) % frequency == 0:\n",
      "-                yield epoch, iteration\n",
      "-\n",
      "-def get_validator_individual(datasets,x,y, error, batch_size=1):\n",
      "-    assert batch_size==1\n",
      "-    _, (valid_set_x, valid_set_y), _ = datasets\n",
      "-    if type(valid_set_x) == list:\n",
      "-        n_valid_batches = len(valid_set_x) // batch_size\n",
      "-    else:\n",
      "-        n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] // batch_size\n",
      "-        valid_set_x = valid_set_x.get_value(borrow=True)\n",
      "-        valid_set_y = valid_set_y.eval()\n",
      "-    \n",
      "-    if callable(error):\n",
      "-        validate_model = error\n",
      "-    else:\n",
      "-        validate_model = theano.function(\n",
      "-            [x, y],\n",
      "-            error   \n",
      "-        )\n",
      "-    \n",
      "-    def validate():\n",
      "-        #TODO: some notion of minibatches?\n",
      "-        validation_losses = [validate_model([x], [y]) for x,y in zip(valid_set_x, valid_set_y)]\n",
      "-        return np.mean(validation_losses)\n",
      "-    \n",
      "-    return validate\n",
      "-\n",
      "-def get_tester_individual(datasets,x,y, error, batch_size=1):\n",
      "-    assert batch_size == 1\n",
      "-    _, _, (test_set_x, test_set_y) = datasets\n",
      "-    if type(test_set_x) == list:\n",
      "-        n_test_batches = len(test_set_x) // batch_size\n",
      "-    else:\n",
      "-        n_test_batches = test_set_x.get_value(borrow=True).shape[0] // batch_size\n",
      "-        test_set_x = test_set_x.get_value(borrow=True)\n",
      "-        test_set_y = test_set_y.eval()\n",
      "-    \n",
      "-    if callable(error):\n",
      "-        test_model = error\n",
      "-    else:\n",
      "-        test_model = theano.function(\n",
      "-            [x,y],\n",
      "-            error  \n",
      "-        )\n",
      "-    \n",
      "-    def test():\n",
      "-        test_losses = [test_model([x],[y]) for x,y in zip(test_set_x, test_set_y)]\n",
      "-        return np.mean(test_losses)\n",
      "-    \n",
      "-    return test\n",
      "-\n",
      "-def get_updates(cost, params, learning_rate=0.13):\n",
      "-    updates = set()\n",
      "-    for param in params:\n",
      "-        updates.add((param, param - learning_rate * T.grad(cost, param)))\n",
      "-    return updates\n",
      "-\n",
      "-def param_values(params):\n",
      "-    return [param.get_value() for param in params]\n",
      "-\n",
      "-def set_params(params, values):\n",
      "-    for param, value in zip(params, values):\n",
      "-        param.set_value(value)\n",
      "-\n",
      "-class LogisticRegression(object):\n",
      "-    def __init__(self, input, n_in, n_out):\n",
      "-        self.W = theano.shared(value=np.zeros((n_in, n_out), dtype=floatX), name=\"W\", borrow=True)\n",
      "-        self.b = theano.shared(value=np.zeros((n_out,), dtype=floatX), name=\"b\", borrow=True)\n",
      "-        \n",
      "-        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
      "-        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
      "-        \n",
      "-        self.params = [self.W, self.b]\n",
      "-        self.input = input\n",
      "-        \n",
      "-    def negative_log_likelihood(self, y):\n",
      "-        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
      "-    \n",
      "-    def errors(self, y):\n",
      "-        assert y.ndim == self.y_pred.ndim\n",
      "-        assert y.dtype.startswith('int')\n",
      "-        return T.mean(T.neq(self.y_pred, y))\n",
      "-    \n",
      "+    def __init__(self, max_epochs=float('inf')):\n",
      "+        self.max_epochs = max_epochs\n",
      "+    \n",
      "+    def run(self, iterator):\n",
      "+        global jobtracker\n",
      "+        for epoch, iteration in iterator:\n",
      "+            jobtracker.checkpoint()\n",
      "+            if epoch > self.max_epochs:\n",
      "+                break\n",
      "+\n",
      "+class ValidationCheckpointer(Checkpointer):\n",
      "+    def __init__(self, validate, improvement_threshold=0.995):\n",
      "+        self.validate = validate\n",
      "+        self.improvement_threshold = improvement_threshold\n",
      "+    \n",
      "+    def run(self, iterator):\n",
      "+        global jobtracker\n",
      "+        best_validation_loss = np.inf\n",
      "+        for epoch, iteration in iterator:\n",
      "+            this_validation_loss = self.validate()\n",
      "+            if this_validation_loss < best_validation_loss:\n",
      "+                if this_validation_loss < best_validation_loss * self.improvement_threshold:\n",
      "+                    print(\"Epoch {} iteration {}\".format(epoch, iteration))\n",
      "+                    best_validation_loss = this_validation_loss\n",
      "+                    jobtracker.checkpoint()\n",
      "+\n",
      "+            print(\"Epoch {} current validation loss {}\".format(epoch, this_validation_loss))\n",
      "+\n",
      "+class NeuralUnit(object):\n",
      "+    def __init__(self): pass\n",
      "     @property\n",
      "     def param_values(self):\n",
      "-        return param_values(self.params)\n",
      "+        return [param.get_value() for param in self.params]\n",
      "     \n",
      "     @param_values.setter\n",
      "-    def param_values(self, val):\n",
      "-        set_params(self.params, val)\n",
      "-\n",
      "-class HiddenLayer(object):\n",
      "-    def __init__(self, rng, input, n_in, n_out, W=None, b=None, activation=T.tanh):\n",
      "-        self.input = input\n",
      "-        \n",
      "-        if W is None:\n",
      "-            W_values = np.asarray(\n",
      "-                rng.uniform(\n",
      "-                    low=-np.sqrt(6. / (n_in + n_out)),\n",
      "-                    high=np.sqrt(6. / (n_in + n_out)),\n",
      "-                    size=(n_in, n_out)\n",
      "-                ),\n",
      "-                dtype=floatX\n",
      "-            )\n",
      "-            if activation == T.nnet.sigmoid:\n",
      "-                W_values *= 4\n",
      "-\n",
      "-            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "-\n",
      "-        if b is None:\n",
      "-            b_values = np.zeros((n_out,), dtype=floatX)\n",
      "-            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "-            \n",
      "-        self.W, self.b = W, b\n",
      "-        \n",
      "-        lin_output = T.dot(self.input, self.W) + self.b\n",
      "-        self.output = lin_output if activation is None else activation(lin_output)\n",
      "-        \n",
      "-        self.params = [self.W, self.b]\n",
      "-    \n",
      "-    @property\n",
      "-    def param_values(self):\n",
      "-        return param_values(self.params)\n",
      "-    \n",
      "-    @param_values.setter\n",
      "-    def param_values(self, val):\n",
      "-        set_params(self.params, val)\n",
      "-\n",
      "-class MLP(object):\n",
      "-    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "-        self.input = input\n",
      "-        \n",
      "-        self.hidden_layer = HiddenLayer(rng, input, n_in=n_in, n_out=n_hidden, activation=T.tanh)\n",
      "-        self.log_regression_layer = LogisticRegression(self.hidden_layer.output, n_in=n_hidden, n_out=n_out)\n",
      "-        \n",
      "-        self.negative_log_likelihood = self.log_regression_layer.negative_log_likelihood\n",
      "-        self.errors = self.log_regression_layer.errors\n",
      "-        \n",
      "-        self.L1 = abs(self.hidden_layer.W).sum() + abs(self.log_regression_layer.W.sum())\n",
      "-        self.L2 = (self.hidden_layer.W ** 2).sum() + (self.log_regression_layer.W ** 2).sum()\n",
      "-        \n",
      "-        self.params = self.hidden_layer.params + self.log_regression_layer.params\n",
      "-        \n",
      "-    @property\n",
      "-    def param_values(self):\n",
      "-        return param_values(self.params)\n",
      "-    \n",
      "-    @param_values.setter\n",
      "-    def param_values(self, val):\n",
      "-        set_params(self.params, val)\n",
      "-\n",
      "-def contextwin(l, win):\n",
      "-    '''\n",
      "-    win :: int corresponding to the size of the window\n",
      "-    given a list of indexes composing a sentence\n",
      "-    l :: array containing the word indexes\n",
      "-    it will return a list of list of indexes corresponding\n",
      "-    to context windows surrounding each word in the sentence\n",
      "-    '''\n",
      "-    assert (win % 2) == 1\n",
      "-    assert win >= 1\n",
      "-    l = list(l)\n",
      "-\n",
      "-    lpadded = win // 2 * [-1] + l + win // 2 * [-1]\n",
      "-    out = [lpadded[i:(i + win)] for i in range(len(l))]\n",
      "-\n",
      "-    assert len(out) == len(l)\n",
      "-    return out\n",
      "-\n",
      "-class RNNSLU(object):\n",
      "-    def __init__(self, input, dim_hidden, num_classes, num_embeddings, dim_embeddings, num_context):\n",
      "-        self.embeddings = theano.shared(name='embeddings',\n",
      "-                                        value=0.2 * np.random.uniform(-1.0, 1.0, (num_embeddings+1, dim_embeddings))\n",
      "-                                       .astype(floatX))\n",
      "-        self.Wx = theano.shared(name='Wx',\n",
      "-            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_embeddings * num_context, dim_hidden))\n",
      "-            .astype(floatX))\n",
      "-        \n",
      "-        self.Wh = theano.shared(name='Wh',\n",
      "-            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_hidden, dim_hidden))\n",
      "-            .astype(floatX))\n",
      "-        \n",
      "-        self.W = theano.shared(name='W',\n",
      "-            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_hidden, num_classes))\n",
      "-            .astype(floatX))\n",
      "-        \n",
      "-        self.bh = theano.shared(name='bh', value=np.zeros(dim_hidden, dtype=floatX))\n",
      "-        self.b = theano.shared(name='b', value=np.zeros(num_classes, dtype=floatX))\n",
      "-        self.h0 = theano.shared(name='h0', value=np.zeros(dim_hidden, dtype=floatX))\n",
      "-        \n",
      "-        self.params = [self.embeddings, self.Wx, self.Wh, self.W, self.bh, self.b, self.h0]\n",
      "-        \n",
      "-        #idxs = T.imatrix('idxs') # (words in sentence) x (num_context)\n",
      "-        self.input = input\n",
      "-        idxs = input[0,:,:]\n",
      "-        x = self.x = self.embeddings[idxs].reshape((idxs.shape[0], dim_embeddings * num_context))\n",
      "-        \n",
      "-        def recurrence(x_t, h_t_minus_1):\n",
      "-            h_t = T.nnet.sigmoid(T.dot(x_t, self.Wx) + T.dot(h_t_minus_1, self.Wh) + self.bh)\n",
      "-            s_t = T.nnet.softmax(T.dot(h_t, self.W) + self.b)\n",
      "-            return [h_t, s_t]\n",
      "-        \n",
      "-        [h, s], _ = theano.scan(fn=recurrence,\n",
      "-                               sequences=x,\n",
      "-                               outputs_info=[self.h0, None],\n",
      "-                               n_steps=x.shape[0])\n",
      "-        self.p_y_given_x_sentence = s[:, 0, :]\n",
      "-        self.y_pred = T.argmax(self.p_y_given_x_sentence, axis=1)\n",
      "-        \n",
      "-        self.classify = theano.function(inputs=[idxs], outputs=self.y_pred)\n",
      "-        \n",
      "-        self.normalize = theano.function(inputs=[],\n",
      "-                                        updates={self.embeddings : self.embeddings / \n",
      "-                                                 T.sqrt((self.embeddings**2)).sum(axis=1).dimshuffle(0, 'x')})\n",
      "-        \n",
      "-    def sentence_negative_log_likelihood(self, y_sentence):\n",
      "-        return -T.mean(T.log(self.p_y_given_x_sentence)[T.arange(self.x.shape[0]), y_sentence])\n",
      "-    \n",
      "-    def negative_log_likelihood(self, y):\n",
      "-        return self.sentence_negative_log_likelihood(y[0,:])\n",
      "-        \n",
      "-    def train(self, x, y, window_size, learning_rate):\n",
      "-        cwords = contextwin(x, window_size)\n",
      "-        words = [np.asarray(x).astype('int32') for x in cwords]\n",
      "-        labels = y\n",
      "-        \n",
      "-        self.sentence_train(words, labels, learning_rate)\n",
      "-        self.normalize()\n",
      "-        \n",
      "-    @property\n",
      "-    def param_values(self):\n",
      "-        return param_values(self.params)\n",
      "-    \n",
      "-    @param_values.setter\n",
      "-    def param_values(self, val):\n",
      "-        set_params(self.params, val)\n",
      "-\n",
      "-class LSTMSLU(object):\n",
      "-    def __init__(self, input, dim_hidden, num_classes, num_embeddings, dim_embeddings, num_context):\n",
      "-        self.embeddings = theano.shared(name='embeddings',\n",
      "-                                        value=0.2 * np.random.uniform(-1.0, 1.0, (num_embeddings+1, dim_embeddings))\n",
      "-                                       .astype(floatX))\n",
      "-        self.Wx_c = theano.shared(name='Wx_c',\n",
      "-            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_embeddings * num_context, dim_hidden))\n",
      "-            .astype(floatX))\n",
      "-        \n",
      "-        self.Wh_c = theano.shared(name='Wh_c',\n",
      "-            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_hidden, dim_hidden))\n",
      "-            .astype(floatX))\n",
      "-        \n",
      "-        self.Wx_i = theano.shared(name='Wx_i',\n",
      "-            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_embeddings * num_context, dim_hidden))\n",
      "-            .astype(floatX))\n",
      "-        \n",
      "-        self.Wh_i = theano.shared(name='Wh_i',\n",
      "-            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_hidden, dim_hidden))\n",
      "-            .astype(floatX))\n",
      "-        \n",
      "-        self.Wx_o = theano.shared(name='Wx_o',\n",
      "-            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_embeddings * num_context, dim_hidden))\n",
      "-            .astype(floatX))\n",
      "-        \n",
      "-        self.Wh_o = theano.shared(name='Wh_o',\n",
      "-            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_hidden, dim_hidden))\n",
      "-            .astype(floatX))\n",
      "-        \n",
      "-        self.W = theano.shared(name='W',\n",
      "-            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_hidden, num_classes))\n",
      "-            .astype(floatX))\n",
      "-        \n",
      "-        self.bh_c = theano.shared(name='bh_c', value=np.zeros(dim_hidden, dtype=floatX))\n",
      "-        self.bh_i = theano.shared(name='bh_i', value=np.zeros(dim_hidden, dtype=floatX))\n",
      "-        self.bh_o = theano.shared(name='bh_o', value=np.zeros(dim_hidden, dtype=floatX))\n",
      "-        self.b = theano.shared(name='b', value=np.zeros(num_classes, dtype=floatX))\n",
      "-        self.h0 = theano.shared(name='h0', value=np.zeros(dim_hidden, dtype=floatX))\n",
      "-        self.C0 = theano.shared(name='C0', value=np.zeros(dim_hidden, dtype=floatX))\n",
      "-        \n",
      "-        self.params = [self.embeddings,\n",
      "-                       self.Wx_c, self.Wh_c,\n",
      "-                       self.Wx_i, self.Wh_i,\n",
      "-                       self.Wx_o, self.Wh_o,\n",
      "-                       self.W,\n",
      "-                       self.bh_c, self.bh_i, self.bh_o,\n",
      "-                       self.b,\n",
      "-                       self.h0, self.C0]\n",
      "-        \n",
      "-        #idxs = T.imatrix('idxs') # (words in sentence) x (num_context)\n",
      "-        self.input = input\n",
      "-        idxs = input[0,:,:]\n",
      "-        x = self.x = self.embeddings[idxs].reshape((idxs.shape[0], dim_embeddings * num_context))\n",
      "-        \n",
      "-        def recurrence(x_t, h_t_minus_1, C_t_minus_1):\n",
      "-            C_candidate_t = T.tanh(T.dot(x_t, self.Wx_c) + T.dot(h_t_minus_1, self.Wh_c) + self.bh_c)\n",
      "-            #input gate\n",
      "-            i_t = T.nnet.sigmoid(T.dot(x_t, self.Wx_i) + T.dot(h_t_minus_1, self.Wh_i) + self.bh_i)\n",
      "-            f_t = 1.0 # no forget gate, yet\n",
      "-            \n",
      "-            C_t = i_t * C_candidate_t + f_t * C_t_minus_1\n",
      "-            \n",
      "-            #output gate\n",
      "-            o_t = T.nnet.sigmoid(T.dot(x_t, self.Wx_o) + T.dot(h_t_minus_1, self.Wh_o) + self.bh_o)\n",
      "-            \n",
      "-            h_t = o_t * T.tanh(C_t)\n",
      "-            s_t = T.nnet.softmax(T.dot(h_t, self.W) + self.b)\n",
      "-            return [h_t, s_t, C_t]\n",
      "-        \n",
      "-        [h, s, C], _ = theano.scan(fn=recurrence,\n",
      "-                               sequences=x,\n",
      "-                               outputs_info=[self.h0, None, self.C0],\n",
      "-                               n_steps=x.shape[0])\n",
      "-        self.p_y_given_x_sentence = s[:, 0, :]\n",
      "-        self.y_pred = T.argmax(self.p_y_given_x_sentence, axis=1)\n",
      "-        \n",
      "-        self.classify = theano.function(inputs=[idxs], outputs=self.y_pred)\n",
      "-        \n",
      "-        self.normalize = theano.function(inputs=[],\n",
      "-                                        updates={self.embeddings : self.embeddings / \n",
      "-                                                 T.sqrt((self.embeddings**2)).sum(axis=1).dimshuffle(0, 'x')})\n",
      "-        \n",
      "-    def sentence_negative_log_likelihood(self, y_sentence):\n",
      "-        return -T.mean(T.log(self.p_y_given_x_sentence)[T.arange(self.x.shape[0]), y_sentence])\n",
      "-    \n",
      "-    def negative_log_likelihood(self, y):\n",
      "-        return self.sentence_negative_log_likelihood(y[0,:])\n",
      "-        \n",
      "-    def train(self, x, y, window_size, learning_rate):\n",
      "-        cwords = contextwin(x, window_size)\n",
      "-        words = [np.asarray(x).astype('int32') for x in cwords]\n",
      "-        labels = y\n",
      "-        \n",
      "-        self.sentence_train(words, labels, learning_rate)\n",
      "-        self.normalize()\n",
      "-        \n",
      "-    @property\n",
      "-    def param_values(self):\n",
      "-        return param_values(self.params)\n",
      "-    \n",
      "-    @param_values.setter\n",
      "-    def param_values(self, val):\n",
      "-        set_params(self.params, val)\n",
      "+    def param_values(self, values):\n",
      "+        for param, value in zip(self.params, values):\n",
      "+            param.set_value(value)  \n",
      " \n",
      " def concat(seq2d, pre=None, post=None):\n",
      "     res = seq2d\n",
      "@@ -524,32 +289,37 @@\n",
      "         res = T.concatenate([res, T.ones_like(seq2d[:, 0]).dimshuffle((0, 'x')) * post], axis=1)\n",
      "     return res\n",
      " \n",
      "-class LSTMEncDec(object):\n",
      "-    def __init__(self, input, dim_hidden, num_classes, num_embeddings, dim_embeddings, num_context=1, reverse_input=True):\n",
      "+class LSTMEncDec(NeuralUnit):\n",
      "+    def __init__(self, input, dim_hidden, num_classes, num_embeddings, dim_embeddings, num_context=1,\n",
      "+                 reverse_input=True):\n",
      "         self.num_classes = num_classes\n",
      "         \n",
      "-        # Add 3 to num_embeddings to account for the following classes:\n",
      "-        # -1: </in>\n",
      "-        # -2: <in>\n",
      "-        # -3: <None>\n",
      "+        # Add 1 to num_embeddings to account for the following classe:\n",
      "+        # -1: <in>\n",
      "         self.embeddings = theano.shared(name='embeddings',\n",
      "-                                        value=0.2 * np.random.uniform(-1.0, 1.0, (num_embeddings+3, dim_embeddings))\n",
      "+                                        value=0.2 * np.random.uniform(-1.0, 1.0, (num_embeddings+1, dim_embeddings))\n",
      "                                        .astype(floatX))\n",
      "         \n",
      "         self.input = input\n",
      "         if not reverse_input:\n",
      "-            self.encoder_input = concat(input, pre=-2)\n",
      "+            self.encoder_input = concat(input, pre=-1)\n",
      "         else:\n",
      "-            self.encoder_input = concat(input, post=-2)\n",
      "-        \n",
      "-        # Construct input to the decoder: </in>, <None> ... <None>\n",
      "-        self.decoder_input = concat(T.ones((input.shape[0], 40), dtype='int32') * -3, pre=-1)\n",
      "-        \n",
      "-        #idxs = T.imatrix('idxs') # (words in sentence) x (num_context)\n",
      "-\n",
      "-        # Input, sentence 0, all chars\n",
      "+            self.encoder_input = concat(input, post=-1)\n",
      "         idxs = self.encoder_input[0,:]\n",
      "-        #x = self.x = self.embeddings[idxs]     \n",
      "+        \n",
      "+        \n",
      "+        # Temporary variable for \"output\"\n",
      "+        # Add 2 additional output classes to account for:\n",
      "+        #   -2: <out>\n",
      "+        #   -1: </out>\n",
      "+        self.dim_decoder_in = num_classes + 2\n",
      "+        \n",
      "+        self.temp_output = T.imatrix('temp_output') # (words) x (phonemes in output), not including a stop token\n",
      "+        \n",
      "+        # Construct input to the decoder: prepend <out>\n",
      "+        self.decoder_input = concat(self.temp_output, pre=-2)\n",
      "+        decoder_embeddings = T.eye(self.dim_decoder_in, dtype='int32') # mapping to one-hot code\n",
      "+        decoder_idxs = self.decoder_input[0,:]    \n",
      "         \n",
      "         self.Wx_c = theano.shared(name='Wx_c',\n",
      "             value=0.2 * np.random.uniform(-1.0, 1.0, (dim_embeddings * num_context, dim_hidden))\n",
      "@@ -573,16 +343,45 @@\n",
      "         \n",
      "         self.Wh_o = theano.shared(name='Wh_o',\n",
      "             value=0.2 * np.random.uniform(-1.0, 1.0, (dim_hidden, dim_hidden))\n",
      "-            .astype(floatX))\n",
      "-        \n",
      "-        self.W = theano.shared(name='W',\n",
      "-            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_hidden, num_classes + 1))\n",
      "             .astype(floatX))\n",
      "         \n",
      "         self.bh_c = theano.shared(name='bh_c', value=np.zeros(dim_hidden, dtype=floatX))\n",
      "         self.bh_i = theano.shared(name='bh_i', value=np.zeros(dim_hidden, dtype=floatX))\n",
      "         self.bh_o = theano.shared(name='bh_o', value=np.zeros(dim_hidden, dtype=floatX))\n",
      "-        self.b = theano.shared(name='b', value=np.zeros(num_classes + 1, dtype=floatX))\n",
      "+        \n",
      "+        self.Wx_c_dec = theano.shared(name='Wx_c_dec',\n",
      "+            value=0.2 * np.random.uniform(-1.0, 1.0, (self.dim_decoder_in, dim_hidden))\n",
      "+            .astype(floatX))\n",
      "+        \n",
      "+        self.Wh_c_dec = theano.shared(name='Wh_c_dec',\n",
      "+            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_hidden, dim_hidden))\n",
      "+            .astype(floatX))\n",
      "+        \n",
      "+        self.Wx_i_dec = theano.shared(name='Wx_i_dec',\n",
      "+            value=0.2 * np.random.uniform(-1.0, 1.0, (self.dim_decoder_in, dim_hidden))\n",
      "+            .astype(floatX))\n",
      "+        \n",
      "+        self.Wh_i_dec = theano.shared(name='Wh_i_dec',\n",
      "+            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_hidden, dim_hidden))\n",
      "+            .astype(floatX))\n",
      "+        \n",
      "+        self.Wx_o_dec = theano.shared(name='Wx_o_dec',\n",
      "+            value=0.2 * np.random.uniform(-1.0, 1.0, (self.dim_decoder_in, dim_hidden))\n",
      "+            .astype(floatX))\n",
      "+        \n",
      "+        self.Wh_o_dec = theano.shared(name='Wh_o_dec',\n",
      "+            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_hidden, dim_hidden))\n",
      "+            .astype(floatX))\n",
      "+        \n",
      "+        self.bh_c_dec = theano.shared(name='bh_c_dec', value=np.zeros(dim_hidden, dtype=floatX))\n",
      "+        self.bh_i_dec = theano.shared(name='bh_i_dec', value=np.zeros(dim_hidden, dtype=floatX))\n",
      "+        self.bh_o_dec = theano.shared(name='bh_o_dec', value=np.zeros(dim_hidden, dtype=floatX))\n",
      "+        \n",
      "+        \n",
      "+        self.W = theano.shared(name='W',\n",
      "+            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_hidden, self.dim_decoder_in))\n",
      "+            .astype(floatX))\n",
      "+        self.b = theano.shared(name='b', value=np.zeros(self.dim_decoder_in, dtype=floatX))\n",
      "         self.h0 = theano.shared(name='h0', value=np.zeros(dim_hidden, dtype=floatX))\n",
      "         self.C0 = theano.shared(name='C0', value=np.zeros(dim_hidden, dtype=floatX))\n",
      "         \n",
      "@@ -590,8 +389,14 @@\n",
      "                        self.Wx_c, self.Wh_c,\n",
      "                        self.Wx_i, self.Wh_i,\n",
      "                        self.Wx_o, self.Wh_o,\n",
      "+                       self.bh_c, self.bh_i, self.bh_o,\n",
      "+                       \n",
      "+                       self.Wx_c_dec, self.Wh_c_dec,\n",
      "+                       self.Wx_i_dec, self.Wh_i_dec,\n",
      "+                       self.Wx_o_dec, self.Wh_o_dec,\n",
      "+                       self.bh_c_dec, self.bh_i_dec, self.bh_o_dec,\n",
      "+                       \n",
      "                        self.W,\n",
      "-                       self.bh_c, self.bh_i, self.bh_o,\n",
      "                        self.b,\n",
      "                        self.h0, self.C0]\n",
      "         \n",
      "@@ -610,15 +415,15 @@\n",
      "             return [h_t, C_t]\n",
      "         \n",
      "         def decoder_recurrence(x_t, h_t_minus_1, C_t_minus_1):\n",
      "-            C_candidate_t = T.tanh(T.dot(x_t, self.Wx_c) + T.dot(h_t_minus_1, self.Wh_c) + self.bh_c)\n",
      "+            C_candidate_t = T.tanh(T.dot(x_t, self.Wx_c_dec) + T.dot(h_t_minus_1, self.Wh_c_dec) + self.bh_c_dec)\n",
      "             #input gate\n",
      "-            i_t = T.nnet.sigmoid(T.dot(x_t, self.Wx_i) + T.dot(h_t_minus_1, self.Wh_i) + self.bh_i)\n",
      "+            i_t = T.nnet.sigmoid(T.dot(x_t, self.Wx_i_dec) + T.dot(h_t_minus_1, self.Wh_i_dec) + self.bh_i_dec)\n",
      "             f_t = 1.0 # no forget gate, yet\n",
      "             \n",
      "             C_t = i_t * C_candidate_t + f_t * C_t_minus_1\n",
      "             \n",
      "             #output gate\n",
      "-            o_t = T.nnet.sigmoid(T.dot(x_t, self.Wx_o) + T.dot(h_t_minus_1, self.Wh_o) + self.bh_o)\n",
      "+            o_t = T.nnet.sigmoid(T.dot(x_t, self.Wx_o_dec) + T.dot(h_t_minus_1, self.Wh_o_dec) + self.bh_o_dec)\n",
      "             \n",
      "             h_t = o_t * T.tanh(C_t)\n",
      "             s_t = T.nnet.softmax(T.dot(h_t, self.W) + self.b)\n",
      "@@ -631,80 +436,377 @@\n",
      "                                n_steps=self.encoder_input.shape[1])\n",
      "         \n",
      "         [dec_h, s, dec_C], _ = theano.scan(fn=decoder_recurrence,\n",
      "-                               sequences=self.embeddings[self.decoder_input[0,:]],\n",
      "+                               sequences=decoder_embeddings[decoder_idxs],\n",
      "                                outputs_info=[enc_h[-1,:], None, enc_C[-1,:]],\n",
      "-                               n_steps=41) # Hardcoded!!!\n",
      "+                               n_steps=self.decoder_input.shape[1])\n",
      "         \n",
      "         self.enc_h, self.enc_C, self.dec_h, self.s, self.dec_C = enc_h, enc_C, dec_h, s, dec_C\n",
      "         \n",
      "         self.p_y_given_x_single = s[:, 0, :]\n",
      "-        self.y_pred = T.argmax(self.p_y_given_x_single, axis=1)\n",
      "-        \n",
      "-        self.translate_full = theano.function(inputs=[input], outputs=self.y_pred)\n",
      "+        \n",
      "+        #self.y_pred = T.argmax(self.p_y_given_x_single, axis=1)\n",
      "+        #self.translate_full = theano.function(inputs=[input], outputs=self.y_pred)\n",
      "+        \n",
      "+        # For translating with the system\n",
      "+        h_in = T.vector('h_in')\n",
      "+        C_in = T.vector('C_in')\n",
      "+        prev_in = T.iscalar('prev_in')\n",
      "+        h_out, s_out, C_out = decoder_recurrence(decoder_embeddings[prev_in, :], h_in, C_in)\n",
      "+        \n",
      "+        self.encode = theano.function(inputs=[input],\n",
      "+                                     outputs=[enc_h[-1,:], enc_C[-1, :]])\n",
      "+        \n",
      "+        self.decode_one = theano.function(inputs=[prev_in, h_in, C_in],\n",
      "+                                         outputs=[s_out[0, :], h_out, C_out])\n",
      "         \n",
      "         self.normalize = theano.function(inputs=[],\n",
      "                                         updates={self.embeddings : self.embeddings / \n",
      "                                                  T.sqrt((self.embeddings**2)).sum(axis=1).dimshuffle(0, 'x')})\n",
      "     \n",
      "-    def _single_negative_log_likelihood(self, y_single):\n",
      "-        #y_single is already padded with an </out> token\n",
      "-        likelihoods = self.p_y_given_x_single[T.arange(0, y_single.shape[0]), y_single]\n",
      "+    def negative_log_likelihood(self, y):\n",
      "+        y_padded = concat(y, post=-1) # </out> tag at the end\n",
      "+        \n",
      "+        # During training, feed gold values instead of recurrent network outputs\n",
      "+        p_y_given_x_single = theano.clone(self.p_y_given_x_single, replace={\n",
      "+            self.temp_output: y\n",
      "+        })\n",
      "+        \n",
      "+        # Assume that we only have a single word per batch\n",
      "+        y_single = y_padded[0, :]\n",
      "+        likelihoods = p_y_given_x_single[T.arange(0, y_single.shape[0]), y_single]\n",
      "         return -T.sum(T.log(likelihoods))\n",
      "     \n",
      "-    #def sentence_negative_log_likelihood(self, y_sentence):\n",
      "-    #    return -T.mean(T.log(self.p_y_given_x_sentence)[T.arange(self.x.shape[0]), y_sentence])\n",
      "-    \n",
      "-    def negative_log_likelihood(self, y):\n",
      "-        return self._single_negative_log_likelihood(concat(y, post=-1)[0, :]) # </out> tag at the end\n",
      "-    \n",
      "     def translate(self, x): # Translate a single word\n",
      "-        ys = self.translate_full(np.asarray([x], dtype='int32'))\n",
      "-        try:\n",
      "-            none_idx = nclasses # equivalent to -1\n",
      "-            ys = ys[:np.min(np.where(ys == self.num_classes))] # Until a <None> is hit\n",
      "-        except ValueError: # There are no <None> tokens\n",
      "-            pass\n",
      "-        \n",
      "-        return np.asarray(ys, dtype='int32')\n",
      "-        \n",
      "-    @property\n",
      "-    def param_values(self):\n",
      "-        return param_values(self.params)\n",
      "-    \n",
      "-    @param_values.setter\n",
      "-    def param_values(self, val):\n",
      "-        set_params(self.params, val)\n",
      "+        # TODO: implement beam search\n",
      "+        done_idx = self.dim_decoder_in - 1 # </out>\n",
      "+        \n",
      "+        h, C = self.encode(np.asarray([x], dtype='int32'))\n",
      "+        res = [-2] # <out>\n",
      "+        while len(res) < 42:\n",
      "+            next_distribution, h, C = self.decode_one(res[-1], h, C)\n",
      "+            res.append(next_distribution.argmax())\n",
      "+            \n",
      "+            if res[-1] == done_idx:\n",
      "+                return np.asarray(res[1:-1], dtype='int32')\n",
      "+        \n",
      "+        return np.asarray(res[1:], dtype='int32')\n",
      "+\n",
      "+class LSTMMasking(NeuralUnit):\n",
      "+    def __init__(self, input, input_mask, dim_hidden, num_classes, num_embeddings, dim_embeddings, num_context=1,\n",
      "+                 reverse_input=True, fixed_len=None):\n",
      "+        self.num_classes = num_classes\n",
      "+        \n",
      "+        # Add 1 to num_embeddings to account for the following classe:\n",
      "+        # -1: <in>\n",
      "+        self.embeddings = theano.shared(name='embeddings',\n",
      "+                                        value=0.2 * np.random.uniform(-1.0, 1.0, (num_embeddings+1, dim_embeddings))\n",
      "+                                       .astype(floatX))\n",
      "+        \n",
      "+        self.input = input\n",
      "+        self.input_mask = input_mask\n",
      "+        \n",
      "+        if fixed_len is not None:\n",
      "+            input = T.specify_shape(x, (hparams.batch_size, 20))\n",
      "+            input_mask = T.specify_shape(x_mask, (hparams.batch_size, 20))\n",
      "+        \n",
      "+        if not reverse_input:\n",
      "+            self.encoder_input = concat(input, pre=-1)\n",
      "+            self.encoder_mask = concat(input_mask, pre=1)\n",
      "+        else:\n",
      "+            self.encoder_input = concat(input, post=-1)\n",
      "+            self.encoder_mask = concat(input_mask, post=1)\n",
      "+        idxs = self.encoder_input[:,:]\n",
      "+        \n",
      "+        \n",
      "+        # Temporary variable for \"output\"\n",
      "+        # Add 2 additional output classes to account for:\n",
      "+        #   -2: <out>\n",
      "+        #   -1: </out>\n",
      "+        self.dim_decoder_in = num_classes + 2\n",
      "+        \n",
      "+        self.temp_output = T.imatrix('temp_output') # (words) x (phonemes in output), not including a stop token\n",
      "+        self.temp_output_mask = T.bmatrix('temp_output_mask') # (words) x (max phonemes out), not incl stop token\n",
      "+        \n",
      "+        temp_output, temp_output_mask = self.temp_output, self.temp_output_mask\n",
      "+        if fixed_len is not None:\n",
      "+            temp_output = T.specify_shape(temp_output, (hparams.batch_size, 20))\n",
      "+            temp_output_mask = T.specify_shape(temp_output_mask, (hparams.batch_size, 20))\n",
      "+        \n",
      "+        # Construct input to the decoder: prepend <out>\n",
      "+        self.decoder_input = concat(temp_output, pre=-2)\n",
      "+        self.decoder_mask = concat(temp_output_mask, pre=1)\n",
      "+        decoder_embeddings = T.eye(self.dim_decoder_in, dtype='int32') # mapping to one-hot code\n",
      "+        decoder_idxs = self.decoder_input[:,:]    \n",
      "+        \n",
      "+        self.Wx_c = theano.shared(name='Wx_c',\n",
      "+            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_embeddings * num_context, dim_hidden))\n",
      "+            .astype(floatX))\n",
      "+        \n",
      "+        self.Wh_c = theano.shared(name='Wh_c',\n",
      "+            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_hidden, dim_hidden))\n",
      "+            .astype(floatX))\n",
      "+        \n",
      "+        self.Wx_i = theano.shared(name='Wx_i',\n",
      "+            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_embeddings * num_context, dim_hidden))\n",
      "+            .astype(floatX))\n",
      "+        \n",
      "+        self.Wh_i = theano.shared(name='Wh_i',\n",
      "+            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_hidden, dim_hidden))\n",
      "+            .astype(floatX))\n",
      "+        \n",
      "+        self.Wx_o = theano.shared(name='Wx_o',\n",
      "+            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_embeddings * num_context, dim_hidden))\n",
      "+            .astype(floatX))\n",
      "+        \n",
      "+        self.Wh_o = theano.shared(name='Wh_o',\n",
      "+            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_hidden, dim_hidden))\n",
      "+            .astype(floatX))\n",
      "+        \n",
      "+        self.bh_c = theano.shared(name='bh_c', value=np.zeros((1, dim_hidden), dtype=floatX),\n",
      "+                                      #broadcastable=[True, False]\n",
      "+                                 )\n",
      "+        self.bh_i = theano.shared(name='bh_i', value=np.zeros((1, dim_hidden), dtype=floatX),\n",
      "+                                      #broadcastable=[True, False]\n",
      "+                                 )\n",
      "+        self.bh_o = theano.shared(name='bh_o', value=np.zeros((1, dim_hidden), dtype=floatX),\n",
      "+                                      #broadcastable=[True, False]\n",
      "+                                 )\n",
      "+        \n",
      "+        bh_c_b = T.patternbroadcast(self.bh_c, (True, False))\n",
      "+        bh_i_b = T.patternbroadcast(self.bh_i, (True, False))\n",
      "+        bh_o_b = T.patternbroadcast(self.bh_o, (True, False))\n",
      "+        \n",
      "+        self.Wx_c_dec = theano.shared(name='Wx_c_dec',\n",
      "+            value=0.2 * np.random.uniform(-1.0, 1.0, (self.dim_decoder_in, dim_hidden))\n",
      "+            .astype(floatX))\n",
      "+        \n",
      "+        self.Wh_c_dec = theano.shared(name='Wh_c_dec',\n",
      "+            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_hidden, dim_hidden))\n",
      "+            .astype(floatX))\n",
      "+        \n",
      "+        self.Wx_i_dec = theano.shared(name='Wx_i_dec',\n",
      "+            value=0.2 * np.random.uniform(-1.0, 1.0, (self.dim_decoder_in, dim_hidden))\n",
      "+            .astype(floatX))\n",
      "+        \n",
      "+        self.Wh_i_dec = theano.shared(name='Wh_i_dec',\n",
      "+            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_hidden, dim_hidden))\n",
      "+            .astype(floatX))\n",
      "+        \n",
      "+        self.Wx_o_dec = theano.shared(name='Wx_o_dec',\n",
      "+            value=0.2 * np.random.uniform(-1.0, 1.0, (self.dim_decoder_in, dim_hidden))\n",
      "+            .astype(floatX))\n",
      "+        \n",
      "+        self.Wh_o_dec = theano.shared(name='Wh_o_dec',\n",
      "+            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_hidden, dim_hidden))\n",
      "+            .astype(floatX))\n",
      "+        \n",
      "+        self.bh_c_dec = theano.shared(name='bh_c_dec', value=np.zeros(dim_hidden, dtype=floatX))\n",
      "+        self.bh_i_dec = theano.shared(name='bh_i_dec', value=np.zeros(dim_hidden, dtype=floatX))\n",
      "+        self.bh_o_dec = theano.shared(name='bh_o_dec', value=np.zeros(dim_hidden, dtype=floatX))\n",
      "+        \n",
      "+        \n",
      "+        self.W = theano.shared(name='W',\n",
      "+            value=0.2 * np.random.uniform(-1.0, 1.0, (dim_hidden, self.dim_decoder_in))\n",
      "+            .astype(floatX))\n",
      "+        self.b = theano.shared(name='b', value=np.zeros(self.dim_decoder_in, dtype=floatX))\n",
      "+        self.h0 = theano.shared(name='h0', value=np.zeros((1, dim_hidden), dtype=floatX))\n",
      "+        self.C0 = theano.shared(name='C0', value=np.zeros((1, dim_hidden), dtype=floatX))\n",
      "+        \n",
      "+        self.params = [self.embeddings,\n",
      "+                       self.Wx_c, self.Wh_c,\n",
      "+                       self.Wx_i, self.Wh_i,\n",
      "+                       self.Wx_o, self.Wh_o,\n",
      "+                       self.bh_c, self.bh_i, self.bh_o,\n",
      "+                       \n",
      "+                       self.Wx_c_dec, self.Wh_c_dec,\n",
      "+                       self.Wx_i_dec, self.Wh_i_dec,\n",
      "+                       self.Wx_o_dec, self.Wh_o_dec,\n",
      "+                       self.bh_c_dec, self.bh_i_dec, self.bh_o_dec,\n",
      "+                       \n",
      "+                       self.W,\n",
      "+                       self.b,\n",
      "+                       self.h0, self.C0]\n",
      "+        \n",
      "+        def encoder_recurrence(x_t, mask_t, h_t_minus_1, C_t_minus_1):\n",
      "+            C_candidate_t = T.tanh(T.dot(x_t, self.Wx_c) + T.dot(h_t_minus_1, self.Wh_c) + bh_c_b)\n",
      "+            #input gate\n",
      "+            i_t = T.nnet.sigmoid(T.dot(x_t, self.Wx_i) + T.dot(h_t_minus_1, self.Wh_i) + bh_i_b)\n",
      "+            f_t = 1.0 # no forget gate, yet\n",
      "+            \n",
      "+            C_t = i_t * C_candidate_t + f_t * C_t_minus_1\n",
      "+            \n",
      "+            #output gate\n",
      "+            o_t = T.nnet.sigmoid(T.dot(x_t, self.Wx_o) + T.dot(h_t_minus_1, self.Wh_o) + bh_o_b)\n",
      "+            \n",
      "+            h_t = o_t * T.tanh(C_t)\n",
      "+            \n",
      "+            # We always have an <in> token at the start to read in h0 and C0\n",
      "+            # So if mask is 0, we simply need to pass along the previous step's value unchanged\n",
      "+            bvec = T.ones((1, dim_hidden))\n",
      "+            mask_t = T.dot(mask_t.dimshuffle(0, 'x'), bvec)\n",
      "+            return [h_t * mask_t + h_t_minus_1 * (1-mask_t), C_t * mask_t + C_t_minus_1 * (1-mask_t)]\n",
      "+            #return h_t, C_t\n",
      "+        \n",
      "+        def decoder_recurrence(x_t, h_t_minus_1, C_t_minus_1):\n",
      "+            C_candidate_t = T.tanh(T.dot(x_t, self.Wx_c_dec) + T.dot(h_t_minus_1, self.Wh_c_dec) + self.bh_c_dec)\n",
      "+            #input gate\n",
      "+            i_t = T.nnet.sigmoid(T.dot(x_t, self.Wx_i_dec) + T.dot(h_t_minus_1, self.Wh_i_dec) + self.bh_i_dec)\n",
      "+            f_t = 1.0 # no forget gate, yet\n",
      "+            \n",
      "+            C_t = i_t * C_candidate_t + f_t * C_t_minus_1\n",
      "+            \n",
      "+            #output gate\n",
      "+            o_t = T.nnet.sigmoid(T.dot(x_t, self.Wx_o_dec) + T.dot(h_t_minus_1, self.Wh_o_dec) + self.bh_o_dec)\n",
      "+            \n",
      "+            h_t = o_t * T.tanh(C_t)\n",
      "+            s_t = T.nnet.softmax(T.dot(h_t, self.W) + self.b)\n",
      "+            return [h_t, s_t, C_t]\n",
      "+        \n",
      "+        def greedy_decoder_recurrence(h_t_minus_1, s_t_minus_1, C_t_minus_1):\n",
      "+            s_t_minus_1 = T.ones((h_t_minus_1.shape[0], 1), dtype='int32') * -2\n",
      "+            h_t, s_t, C_t = decoder_recurrence(s_t_minus_1, h_t_minus_1, C_t_minus_1)\n",
      "+            #s_t = T.patternbroadcast(T.argmax(s_t, axis=1), (False, False))\n",
      "+            s_t = T.ones((h_t_minus_1.shape[0], 1), dtype='int32') * -2\n",
      "+            return [h_t, s_t, C_t]\n",
      "+        \n",
      "+        # TODO(nikita): actually find a way to broadcast the initial state along this dimension\n",
      "+        broadcasting_vector = T.ones((x.shape[0], 1))\n",
      "+        \n",
      "+        [enc_h, enc_C], _ = theano.scan(fn=encoder_recurrence,\n",
      "+                               sequences=[self.embeddings[idxs].dimshuffle(1, 0, 2),\n",
      "+                                          self.encoder_mask.dimshuffle(1, 0)],\n",
      "+                               outputs_info=[T.dot(broadcasting_vector, self.h0),\n",
      "+                                             T.dot(broadcasting_vector, self.C0)],\n",
      "+                               go_backwards=reverse_input,\n",
      "+                               n_steps=self.encoder_input.shape[1])\n",
      "+        \n",
      "+        [dec_h, s, dec_C], _ = theano.scan(fn=decoder_recurrence,\n",
      "+                               sequences=decoder_embeddings[decoder_idxs].dimshuffle(1,0,2),\n",
      "+                               outputs_info=[enc_h[-1,:], None, enc_C[-1,:]],\n",
      "+                               n_steps=self.decoder_input.shape[1])\n",
      "+        \n",
      "+        \"\"\"[greedy_dec_h, greedy_s, greedy_dec_C], _ = theano.scan(fn=greedy_decoder_recurrence,\n",
      "+                       outputs_info=[enc_h[-1,:], broadcasting_vector * -2, enc_C[-1,:]],\n",
      "+                       n_steps=20)\n",
      "+        \"\"\"\n",
      "+        self.enc_h, self.enc_C, self.dec_h, self.s, self.dec_C = enc_h, enc_C, dec_h, s, dec_C\n",
      "+        \n",
      "+        self.p_y_given_x = s.dimshuffle(1, 0, 2)\n",
      "+        self.p_y_given_x_single = self.p_y_given_x[0, :, :]\n",
      "+        \n",
      "+        #self.y_pred = T.argmax(self.p_y_given_x_single, axis=1)\n",
      "+        #self.translate_full = theano.function(inputs=[input], outputs=self.y_pred)\n",
      "+        \n",
      "+        # For translating with the system (NEW WAY)\n",
      "+        \"\"\"self.translate_full = theano.function(\n",
      "+            inputs=[self.input, self.input_mask],\n",
      "+            outputs=[greedy_s]\n",
      "+        )\n",
      "+        \"\"\"\n",
      "+        # For translating with the system (OLD WAY)\n",
      "+        h_in = T.vector('h_in')\n",
      "+        C_in = T.vector('C_in')\n",
      "+        prev_in = T.iscalar('prev_in')\n",
      "+        h_out, s_out, C_out = decoder_recurrence(decoder_embeddings[prev_in, :], h_in, C_in)\n",
      "+        \n",
      "+        self.encode = theano.function(inputs=[self.input, self.input_mask],\n",
      "+                                     outputs=[enc_h[-1,0,:], enc_C[-1, 0, :]])\n",
      "+        \n",
      "+        self.decode_one = theano.function(inputs=[prev_in, h_in, C_in],\n",
      "+                                         outputs=[s_out[0, :], h_out, C_out])\n",
      "+        \n",
      "+        self.normalize = theano.function(inputs=[],\n",
      "+                                        updates={self.embeddings : self.embeddings / \n",
      "+                                                 T.sqrt((self.embeddings**2)).sum(axis=1).dimshuffle(0, 'x')})\n",
      "+    \n",
      "+    \"\"\"def single_negative_log_likelihood(self, y): # for one word only\n",
      "+        y_padded = concat(y, post=-1) # </out> tag at the end\n",
      "+        \n",
      "+        # During training, feed gold values instead of recurrent network outputs\n",
      "+        p_y_given_x_single = theano.clone(self.p_y_given_x_single, replace={\n",
      "+                self.temp_output: y,\n",
      "+        })\n",
      "+        \n",
      "+        # Assume that we only have a single word per batch\n",
      "+        y_single = y_padded[0, :]\n",
      "+        likelihoods = p_y_given_x_single[T.arange(0, y_single.shape[0]), y_single]\n",
      "+        return -T.sum(T.log(likelihoods))\"\"\"\n",
      "+    \n",
      "+    def negative_log_likelihood(self, y, y_mask):\n",
      "+        \"\"\"\n",
      "+        Assume that y is padded with </out> tags, but that the mask does not cover them.\n",
      "+        \"\"\"\n",
      "+        # During training, feed gold values into the decoder instead of the network outputs\n",
      "+        p_y_given_x = theano.clone(self.p_y_given_x, replace={\n",
      "+                self.temp_output: y,\n",
      "+        })\n",
      "+        \n",
      "+        y_mask = concat(y_mask, pre=1)[:,:-1]\n",
      "+\n",
      "+        likelihoods, _ = theano.map(lambda p_y_given_x_single, y_single, y_mask_single:\n",
      "+                           p_y_given_x_single[T.arange(0,y_single.shape[0]), y_single] * y_mask_single + (1 - y_mask_single),\n",
      "+                   sequences = [p_y_given_x, y, y_mask],\n",
      "+                  )\n",
      "+        return -T.sum(T.log(likelihoods))\n",
      "+    \n",
      "+    def translate(self, x): # Translate a single word\n",
      "+        # TODO: implement beam search\n",
      "+        done_idx = self.dim_decoder_in - 1 # </out>\n",
      "+        \n",
      "+        # HACK: truncate input to length at most 20, because that is required by our current code\n",
      "+        # TODO: avoid hack regarding batch size\n",
      "+        padded_x = np.zeros((hparams.batch_size, 20), dtype='int32')\n",
      "+        padded_mask = np.zeros((hparams.batch_size, 20), dtype='int8')\n",
      "+        lx = min(20, len(x))\n",
      "+        for i in range(hparams.batch_size):\n",
      "+            padded_x[i,:lx] = x[:lx]\n",
      "+            padded_mask[i,:lx] = 1\n",
      "+        h, C = self.encode(padded_x, padded_mask)\n",
      "+        res = [-2] # <out>\n",
      "+        while len(res) < 42:\n",
      "+            next_distribution, h, C = self.decode_one(res[-1], h, C)\n",
      "+            res.append(next_distribution.argmax())\n",
      "+            \n",
      "+            if res[-1] == done_idx:\n",
      "+                return np.asarray(res[1:-1], dtype='int32')\n",
      "+        \n",
      "+        return np.asarray(res[1:], dtype='int32')\n",
      " \n",
      " with open(os.path.join(DATA_PREFIX, 'datasets_cmudict_nopre.pkl'), 'rb') as f:\n",
      "     datasets, chars, phonemes, nclasses, vocsize, num_context = pickle.load(f)\n",
      "+with open(os.path.join(DATA_PREFIX, 'datasets_cmudict_masked_train.pkl'), 'rb') as f:\n",
      "+    masked_train_set = pickle.load(f)\n",
      " x = T.imatrix('idxs') # (words) x (chars in word)\n",
      "+x_mask = T.bmatrix('idxs_mask')\n",
      " y = T.imatrix('y') # (words) x (phonemes in output)\n",
      "-rnn = LSTMEncDec(input=x,\n",
      "-             dim_hidden=hparams.dim_hidden,\n",
      "-             num_classes=nclasses,\n",
      "-             num_embeddings=vocsize,\n",
      "-             dim_embeddings=hparams.dim_embeddings,\n",
      "-                reverse_input=hparams.reverse_input,)\n",
      "+y_mask = T.bmatrix('y_mask') # (words) x (phonemes in output)\n",
      "+rnn = LSTMMasking(input=x,\n",
      "+                  input_mask=x_mask,\n",
      "+                  dim_hidden=hparams.dim_hidden,\n",
      "+                  num_classes=nclasses,\n",
      "+                  num_embeddings=vocsize,\n",
      "+                  dim_embeddings=hparams.dim_embeddings,\n",
      "+                  reverse_input=hparams.reverse_input,\n",
      "+                  #fixed_len=None\n",
      "+                 )\n",
      " jobtracker.register_checkpointer('rnn.param_values')\n",
      "-cost = rnn.negative_log_likelihood(y)\n",
      "-updates = get_updates(cost, rnn.params, learning_rate=hparams.learning_rate)\n",
      "+cost = rnn.negative_log_likelihood(y, y_mask)\n",
      "+optimizer = AdadeltaOptimizer(cost, rnn.params)\n",
      "+jobtracker.register_checkpointer('optimizer.state_values')\n",
      "+\n",
      "+data_iterator = MaskedMinibatchIterator(masked_train_set,\n",
      "+                                        x, x_mask, y, y_mask,\n",
      "+                                        optimizer,\n",
      "+                                        batch_size = hparams.batch_size,\n",
      "+                                        frequency = hparams.frequency)\n",
      "+jobtracker.register_checkpointer('data_iterator.state_values')\n",
      " def validate():\n",
      "     valid_x, valid_y = datasets[1]\n",
      "     errors_valid = [(wer(y, rnn.translate(x)) * len(y), len(y))\n",
      "-                    for x, y in zip(valid_x, valid_y)]\n",
      "+                    for x, y in list(zip(valid_x, valid_y))[:50]]\n",
      "     \n",
      "     \n",
      "     w_err = np.sum(errors_valid, axis=0)\n",
      "-    w_err = w_err[0] / w_err[1]\n",
      "-    \n",
      "-    return w_err\n",
      "-\n",
      "-def test():\n",
      "-    test_x, test_y = datasets[2]\n",
      "-    errors_test = [(wer(y, rnn.translate(x)) * len(y), len(y))\n",
      "-                    for x, y in zip(test_x, test_y)]\n",
      "-    \n",
      "-    w_err = np.sum(errors_test, axis=0)\n",
      "     w_err = w_err[0] / w_err[1]\n",
      "     \n",
      "     return w_err\n",
      "@@ -801,15 +903,4 @@\n",
      "     wer_result = round( (numSub + numDel + numIns) / (float) (len(r)), 3)\n",
      "     return {'WER':wer_result, 'Cor':numCor, 'Sub':numSub, 'Ins':numIns, 'Del':numDel}\n",
      " \n",
      "-trainer = individual_train(datasets, x, y, cost, updates, frequency=hparams.frequency)\n",
      "-\n",
      "-epoch, iteration = 0, 0\n",
      "-jobtracker.register_checkpointer('epoch')\n",
      "-jobtracker.register_checkpointer('iteration')\n",
      "-\n",
      "-def do_train():\n",
      "-    global epoch, iteration\n",
      "-    for epoch, iteration in patience_training(trainer, validate, rnn.params,\n",
      "-                                              patience=hparams.patience, max_epochs=hparams.max_epochs):\n",
      "-        jobtracker.checkpoint()\n",
      "-#Omitted 54 checkpoints+#Omitted 80 checkpoints"
     ]
    }
   ],
   "source": [
    "a = _153\n",
    "b = _154\n",
    "s = difflib.SequenceMatcher(a=a, b=b)\n",
    "\n",
    "starta = 0\n",
    "startb = 0\n",
    "matches = s.get_matching_blocks()\n",
    "non_matches = []\n",
    "while True:\n",
    "    match = matches.pop()\n",
    "    enda = match.a\n",
    "    endb = match.b\n",
    "    non_matches.append((a[starta:enda], b[startb:endb]))\n",
    "    if match.size == 0:\n",
    "        break\n",
    "    starta = enda + match.size\n",
    "    startb = endb + match.size\n",
    "    \n",
    "for va, vb in non_matches:\n",
    "    for line in difflib.unified_diff('\\n'.join(va).splitlines(True), '\\n'.join(vb).splitlines(True)):\n",
    "        print(line, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('56301716006f87241d481184'),\n",
       " 'entry': Entry('56304fc3006f87241d48125c'),\n",
       " 'hostname': 'tomato',\n",
       " 'name': '2015-10-27T17:30:14-nikita@tomato:9245',\n",
       " 'pid': 9245,\n",
       " 'start_time': datetime.datetime(2015, 10, 27, 17, 30, 14, 421000),\n",
       " 'status': 'Dead',\n",
       " 'stop_time': 'unknown',\n",
       " 'user': 'nikita'}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.check_alive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psutil.pid_exists(9245)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Analysis at 0x7fcb90112f28>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(job.analyses)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_135.check_alive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
