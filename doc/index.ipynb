{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbjob - Using Jupyter notebooks for machine learning research\n",
    "*Implements experiment management and lightweight version control.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nbjob` is a Python library that enhances Jupyter notebooks.\n",
    "\n",
    "Features:\n",
    "* Dispatch jobs from Jupyter Notebooks to an ipyparallel cluster\n",
    "* Designed for long-running jobs, such as training a neural network for greater than a day\n",
    "* Store intermediate and final job results\n",
    "* Keep reproducible source code backups for all jobs\n",
    "* Run analyses on job results\n",
    "* Easily run a series of jobs related jobs with different hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "The Jupyter notebook is a powerful tool for interacting with data. It lets you visualize what is happening while developing algorithms that process the data.\n",
    "\n",
    "At the same time, the Jupyter notebook is a way of sharing a computational narrative: that is, slowly building up pieces of an algorithm and explaining how they work on data.\n",
    "\n",
    "However, this view skips the entire process in between. Here Jupyter is limited in two key ways: it doesn't provide a good architecture for farming out parallel jobs when pickle fails, and it doesn't have version control that's well-adapted to the domain of algorithms research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job architecture\n",
    "\n",
    "One key feature of the notebook is that it encourages keeping a lot of information stored as datastructures in memory, as opposed to stored as files on disk. So you end up having data structures that aren't serialized anywhere, and that maybe can't be regenerated because the code cells leading to them have been deleted. You also have functions that exist only in the memory of the notebook, and maybe as source code somewhere inside a cell.\n",
    "\n",
    "This makes it hard to transmit these datastructures to other machines as jobs. Objects in memory which are executable (like functions) or rely on interpreter state (like generators) are not guaranteed to be portable, and in many cases can't be sent across machines. Details of the Python pickle implementation impose further limitations.\n",
    "\n",
    "#### Version control\n",
    "\n",
    "A notebook in the process of being written may be filled with one-time code to visualize a particular piece of data, that will never be needed again. It may also be highly nonlinear, i.e. just executing the notebook from the top down starting with a clean slate may not even run. Sometimes code may be changed in place, obscuring old versions.\n",
    "\n",
    "Jupyter provides checkpointing as a backup against crashes, but this only covers one use of version control. If a piece of code is deleted, and a researcher wants to remember what it did months down the line, this is not well-supported by a simple versioning setup.\n",
    "\n",
    "Similarly, research often involves considering many different code variations in parallel. Whereas for well-understood domains software engineers can have a linear revision history that slowly adds functionality, it is not unnatural research to have many branching paths. Most of these will be swiftly abandoned, but sometimes need to be dug up at a much later date.\n",
    "\n",
    "### Solution\n",
    "\n",
    "These are symptoms of the same problem: storing code and data in memory instead of on-disk. Even if the full history leading to the current state is theoretically available (e.g. in IPython history logs), it is not easy to use.\n",
    "\n",
    "A common solution to this problem is well known: make sure that jobs are submitted in reproducible source-code form, and store the code of any job ever submitted. This solves portability across machines, and versioning any code that may be of interest. Frameworks that do this typically rely on a combination of existing version control systems, shell scripts, and archive files.\n",
    "\n",
    "What makes it hard to adapt the notebook to this kind of workflow out-of-the-box is that a notebook might contain a mix of algorithmic and exploratory code. The algorithmic code is the key to running reproducible jobs, but the exploratory code is there to help the author make sense of the results and find bugs. It is usually this exploratory code that is unreproducible.\n",
    "\n",
    "The key observation is: the author knows which code is exploratory, and which code isn't. Also, the algorithmic code is typically fairly concise, and also more reproducible than the exploratory sections.\n",
    "\n",
    "So this is the general flavor of the proposal: the author annotates algorithmic code, runs the code in jobs (possibly on remote clusters), and this framework keep tracks of versioning the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other things along the way\n",
    "\n",
    "Other features we pick up along the way:\n",
    "* Having jobs run report intermediate and final results, and storing them\n",
    "* Running analyses on the results\n",
    "* Integration with ipycluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this framework\n",
    "\n",
    "This framework, called `nbjob`, is still heavily a work in progress. It is in flux as I adapt it to my personal research needs.\n",
    "\n",
    "Comments welcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have Snippet Collector objects, which you can use to mark cells as non-experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing IPython notebook from nbjob.ipynb\n",
      "importing IPython notebook from Structs.ipynb\n",
      "importing IPython notebook from Parallel Theano.ipynb\n"
     ]
    }
   ],
   "source": [
    "from nbjob import SnippetCollector\n",
    "sc = SnippetCollector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can collect functions and classes by attaching decorators to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@sc\n",
    "def norm(val):\n",
    "    import math\n",
    "    return math.sqrt(val.x**2 + val.y**2)\n",
    "\n",
    "@sc\n",
    "class Point(object):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also collect pieces of code (which run locally to verify the code is correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%snip sc\n",
    "origin = Point(0., 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can spin up jobs relating to these snippets\n",
    "\n",
    "This first requires connecting to both the database (which stores job results), and an IPython cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nbjob import DBWrapper\n",
    "db_wrapper = DBWrapper()\n",
    "\n",
    "import ipyparallel as ipp\n",
    "rc = ipp.Client()\n",
    "\n",
    "worker_view = rc[0] # Pointer to worker 0 in the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can proceed to running jobs on a particular worker in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "job = db_wrapper.create_job(\n",
    "    worker_view, ['norm(origin)'],\n",
    "    sc\n",
    "    )\n",
    "job.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running jobs from multiple snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We support running multiple snippets in series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc2 = SnippetCollector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@sc2\n",
    "def distance(p1, p2):\n",
    "    diff = Point(p1.x - p2.x, p1.y - p2.y)\n",
    "    return norm(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%snip sc2\n",
    "a = Point(1., 2.)\n",
    "b = Point(3., 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start a job to run the code. This is designed to be interactive, so you will get a widget to review the job before it is submitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job = db_wrapper.create_job(\n",
    "    worker_view, ['distance(a, b)'],\n",
    "    sc, sc2 # Note that we use both here!\n",
    "    )\n",
    "job.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have support for parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nbjob import ParamLogger\n",
    "params = ParamLogger()\n",
    "params.x = 5\n",
    "params.y = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%snip sc2\n",
    "c = Point(params.x, params.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the code below, you will be prompted to adjust the parameters for that particular run, if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job = db_wrapper.create_job(\n",
    "    worker_view, ['distance(a, b)'],\n",
    "    sc, sc2,\n",
    "    params=params # Here we introduce the parameters\n",
    "    )\n",
    "job.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the jobs dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You need to download and run the notebook to see the dashboard.\n",
    "import nbjob\n",
    "nbjob.make_default_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpointing and running analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is particular to machine learning, where we have intermediate values of model parameters and we may want to evaluate something over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine the following dummy setup, where we're training a single parameter value to approximate the number `5.0`. However, as a parallel to machine learning, we don't do it in one step. Instead, we have a procedure where the number iteratively approaches its final value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc3 = SnippetCollector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%snip sc3\n",
    "iteration = 0\n",
    "param_values = [0]\n",
    "jobtracker.register_checkpointer('iteration') # Save this for intermediate results\n",
    "jobtracker.register_checkpointer('param_values') # Save this for intermediate results\n",
    "\n",
    "def train():\n",
    "    global iteration, params\n",
    "    while abs(param_values[0] - 5.0) > 1e-5:\n",
    "        iteration += 1\n",
    "        param_values[0] += 0.5 * (5.0 - param_values[0]) # Iteratively approach 5\n",
    "        jobtracker.checkpoint() # Save the value of iteration and params\n",
    "        \n",
    "# The jobtracker variable is a global variable available in all workers, that provides an interface\n",
    "# for communicating back with the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job = db_wrapper.create_job(\n",
    "    worker_view, ['train()'],\n",
    "    sc3,\n",
    "    )\n",
    "job.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have been checkpointing at every training iteration, and we have made it so that the iteration and parameter values are logged at each checkpoint. We can then analyze the incremental results from running our job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_performance():\n",
    "    global iteration, param_values\n",
    "    return {'iteration':iteration,\n",
    "           'error': abs(param_values[0] - 5.0)}\n",
    "\n",
    "analyzer = db_wrapper.create_analyzer(\n",
    "    worker_view,\n",
    "    job,\n",
    "    [test_performance])\n",
    "analyzer.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analyzer gives us a list of dictionaries, each of which has `iteration` and `error` as keys. We can use them to make a plot of error rate over time.\n",
    "\n",
    "The analysis gets a snapshot of the system with the checkpointed variables correctly restored. This means that if the parameters of a neural network are checkpointed, the analysis can run the network on validation or testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc [not really relevant]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross-notebook include shim\n",
    "with open(\"/home/nikita/notebooks/nbinclude.ipynb\") as nbinclude_f: # don't rename nbinclude_f\n",
    "    import nbformat\n",
    "    get_ipython().run_cell(nbformat.read(nbinclude_f, nbformat.NO_CONVERT).cells[0].source)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
